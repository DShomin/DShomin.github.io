<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-10-15T08:18:14-04:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lomit tech blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>GitHub User</name></author><entry><title type="html">Triton hands-on</title><link href="http://localhost:4000/triton/2022/10/04/Triton-hands-on.html" rel="alternate" type="text/html" title="Triton hands-on" /><published>2022-10-04T05:40:00-04:00</published><updated>2022-10-04T05:40:00-04:00</updated><id>http://localhost:4000/triton/2022/10/04/Triton-hands-on</id><content type="html" xml:base="http://localhost:4000/triton/2022/10/04/Triton-hands-on.html"><![CDATA[<blockquote>
  <p>Triton은 Nvidia에서 제공하는 Deploy open-source로 request, response로 제공하는 서비스에 대해 다양한 기능들을 제공한다.(model version 관리, service performance test, service batch scheduler and so on) 본 내용은 Nvidia 측에서 만들어주신 자료 기반으로 작성되었습니다. 해당 내용은 https://github.com/leejinho610/TRT_Triton_HandsOn 를 참고</p>

</blockquote>

<h2 id="1-pytorch-model-convert">1. PyTorch model convert</h2>

<blockquote>
  <p>model을 deploy 시 model file framework가 다양하다. 이번 hands-on에서는 PyTorch 관련한 framework를 다룰 예정이다.(torch-script, Onnx, tensorRT)</p>

  <ul>
    <li>base model</li>
  </ul>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hands-on간에 사용하고자 하는 base model 이다.

```python
from torchvision import models

model = models.wide_resnet101_2(pretrained=True).eval().cuda()
```
</code></pre></div></div>

<ul>
  <li>
    <p>Torch-script</p>

    <p>torch-script는 pytorch model을 보다 가속화하기 위한 framework로 C++에서도 사용이 가능하다. 보다 자세한 내용은 <a href="https://pytorch.org/docs/stable/jit.html">pytorch 공식 documentation</a>을 참고하는 것이 좋을 것 같다.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">script_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">script</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
  <span class="n">script_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'model.pt'</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Onnx</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"actual_input_1"</span><span class="p">]</span>
  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"output_1"</span><span class="p">]</span>
  <span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">).</span><span class="n">cuda</span><span class="p">(),</span> <span class="s">'model.onnx'</span><span class="p">,</span>
                    <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
                    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s">'actual_input_1'</span><span class="p">:{</span><span class="mi">0</span><span class="p">:</span><span class="s">'batch_size'</span><span class="p">},</span> <span class="s">'output_1'</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'batch_size'</span><span class="p">}})</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>tensorRT</p>

    <p>tensorRT는 Nvidia GPU에 최적화하는 툴로 방법이 크게 두 가지가 있다. (1. trtexec 활용 2. torchtrt 활용)</p>

    <p>📢 주의!! tensorRT는 target이 되는 GPU에서 진행해야 한다.</p>

    <p>나쁜 예) tensorRT 변환은 2080TI에서 진행하고 변환한 파일을 3080TI에서 deploy</p>

    <ol>
      <li>
        <p>trtexec</p>

        <blockquote>
          <p>필자가 직접 환경을 설치하여 tensorRT 변환하는 환경을 만들어 봤지만 많이 어려웠으며 결국 실패하였다. 오기로 시작하였지만 무수한 애러를 보고 컴퓨터에게 졌다… 변환하는 것은 docker를 사용하는 것을 추천하며 아래 docker command를 실행하면 쉽게 변환이 가능하다.</p>

        </blockquote>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> docker run <span class="nt">--gpus</span> <span class="s1">'"device=0"'</span> <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">-p</span> 8887:8887 <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/hands_on <span class="o">[</span>nvcr.io/nvidia/pytorch:22.03-py3]<span class="o">(</span>http://nvcr.io/nvidia/pytorch:22.03-py3<span class="o">)</span>
</code></pre></div>        </div>

        <ul>
          <li>TensorRT 변환
            <ul>
              <li>
                <p>using trtexec</p>

                <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  trtexec <span class="se">\</span>
    <span class="nt">--onnx</span><span class="o">=</span>model.onnx <span class="se">\</span>
    <span class="nt">--explicitBatch</span> <span class="se">\</span>
    <span class="nt">--optShapes</span><span class="o">=</span>actual_input_1:16x3x224x224 <span class="se">\</span>
    <span class="nt">--maxShapes</span><span class="o">=</span>actual_input_1:32x3x224x224 <span class="se">\</span>
    <span class="nt">--minShapes</span><span class="o">=</span>actual_input_1:1x3x224x224 <span class="se">\</span>
    <span class="nt">--best</span> <span class="se">\</span>
    <span class="nt">--saveEngine</span><span class="o">=</span>model.plan <span class="se">\</span>
    <span class="nt">--workspace</span><span class="o">=</span>4096
</code></pre></div>                </div>

                <ul>
                  <li>argument description
                    <ul>
                      <li>onnx : model file의 path</li>
                      <li>explicitBatch</li>
                      <li>optShapes : 최적화하고자 하는 batch shape</li>
                      <li>maxShapes : 최대 batch shape</li>
                      <li>minShapes : 최소 batch shape</li>
                      <li>best : network의 precision을 결정하는 argument로 default가 best를 활성화한 것이며 –fp16, –int8, –noTF32 와 같다.</li>
                      <li>saveEngine : tensorRT 변환 후 저장하고자 하는 이름</li>
                      <li>workspace : 최적화를 위하여 할당하고자 하는 memory size이며 이후 버전 부터 삭제예정</li>
                    </ul>
                  </li>
                </ul>

                <hr />

                <ul>
                  <li>
                    <p>trtexec dummy test</p>

                    <blockquote>
                      <p>변환 이후 변환한 tensorRT 파일이 어느정도 성능이 나오는지 한줄 코드로 수행이 가능하다.</p>

                    </blockquote>

                    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 1. dummy input으로 inference 시간 측정</span>
  trtexec <span class="nt">--loadEngine</span><span class="o">=</span>model.plan <span class="nt">--dumpOutput</span>
                    
  <span class="c"># 2. dummy input으로 layer 별 시간 측정</span>
  trtexec <span class="nt">--loadEngine</span><span class="o">=</span>model.plan <span class="nt">--dumpProfile</span>
</code></pre></div>                    </div>
                  </li>
                </ul>
              </li>
              <li>
                <p>torchtrt libray</p>

                <p>python의 library로 제공되는 방법으로 쉽게 적용할 수 있지만 변환 후 performance는 trtexec 보다 떨어지는 것으로 알려져있다.</p>

                <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">#Optional
</span>  <span class="kn">import</span> <span class="nn">torch_tensorrt</span> <span class="k">as</span> <span class="n">torchtrt</span>
                
  <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">wide_resnet101_2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nb">eval</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span>
  <span class="c1">#Or you can load torchscript file directly likes
</span>  <span class="c1">#model = torch.jit.load('ts_model_path')
</span>                
  <span class="n">trt_module</span> <span class="o">=</span> <span class="n">torchtrt</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">torchtrt</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span>
                                  <span class="n">min_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
                                  <span class="n">opt_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
                                  <span class="n">max_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span> <span class="p">)],</span> <span class="n">enabled_precisions</span><span class="o">=</span><span class="p">{</span><span class="n">torch</span><span class="p">.</span><span class="n">half</span><span class="p">})</span>
                
  <span class="n">trt_module</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'test.ts'</span><span class="p">)</span>
</code></pre></div>                </div>
              </li>
              <li>
                <p>summary</p>

                <p>TensorRT는 Nvidia GPU를 활용하여 deploy를 한다면 사용하지 않을 이유가 없을 만큼 최적화를 잘해준다. (network의 precision을 낮추어 조금의 acc 저하는 있지만…) 하지만 변환하면서 문제가 조금씩 있다. 최적화를 지원하지 않는 operation이 있는 경우에는 추가적인 작업이 더 필요하게 된다. 해결할 수 있는 방법으로는 Onnx-Simplifer 라는 tool을 활용하여 해결하는 방법과 직접 cuda 설계하여 최적화하는 방법, torch_tensorrt를 활용하여 변환하는 방법이 있다.</p>

                <ol>
                  <li>Try <a href="https://github.com/daquexian/onnx-simplifier">Onnx-Simplifier</a> 
  <code class="language-plaintext highlighter-rouge">python3 -m onnxsim model.onnx simplified_model.onnx</code></li>
                  <li><a href="https://github.com/NVIDIA/TensorRT">Custom Plugin?</a>, <a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon">Onnx-GraphSurgeon?</a></li>
                  <li>
                    <p>Use Framework integration version (TF-TRT, Torch-TRT)</p>

                    <p>operation 최적화가 안되는 경우에는 operation을 유지시켜서 변환하는 방법</p>
                  </li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h2 id="2-triton">2. Triton</h2>

<blockquote>
  <p>앞서 operation 경량화와 최적화를 통해서 만든 model file을 활용하여 inference API를 만드는 것을 도와주는 툴이다.</p>

</blockquote>

<p>TensorRT 변환과 같이 Triton 또한 환경을 만들기 어려우니 NGC에서 제공해주는 Docker를 적극 활용해보자</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">--gpus</span><span class="o">=</span><span class="s1">'"device=1"'</span> <span class="nt">--rm</span> <span class="nt">-p8000</span>:8000 <span class="nt">-p8001</span>:8001 <span class="nt">-p8002</span>:8002 <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/models:/models nvcr.io/nvidia/tritonserver:22.03-py3 tritonserver <span class="nt">--model-repository</span><span class="o">=</span>/models
</code></pre></div></div>

<p>(주의!! 📢 docker를 build하는 directory에 models 폴더가 있어야한다.)</p>

<ul>
  <li>
    <p>models의 directory</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  models
  ├── onnx_model
  │   ├── 1
  │   │   └── model.onnx
  │   └── config.pbtxt
  ├── torch_model
  │   ├── 1
  │   │   └── model.pt
  │   └── config.pbtxt
  └── trt_model
      ├── 1
      │   └── model.plan
      └── config.pbtxt
</code></pre></div>    </div>

    <p>models 폴더 안에는 제공하고자하는 서비스 이름으로 폴더(onnx_model, torch_model, trt_model)가 있고 그 하위의 폴더는 해당 서비스의 버전 이름이다.(1) 그 하위로는 모델 파일이 들어가있다. pbtxt 확장자 파일은 서비스에 대한 config 정보들이 들어있으며 model file의 종류에 따라서 조금 씩 다르다.</p>
  </li>
  <li>
    <p>using port number</p>
    <ul>
      <li>8000 : inference를 위한 http 포트</li>
      <li>8001 : inference를 위한 grpc 포트</li>
      <li>
        <p>8002 : 서비스 운용간에 서비스 현황들을 모니터링 하기 위한  포트(프로메테우스와 연동할 수 있다.)</p>

        <p><img src="/assets/images/triton-handson/Untitled.png" alt="Untitled" /></p>
      </li>
    </ul>
  </li>
</ul>

<p>위 도커를 실행하게 되면 제공하고자 하는 tritonserver가 실행이 되고 해당 컨테이너가 server로서 활용이 가능하게 된다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># optional</span>
<span class="c"># model의 load하는 형식을 주는 것으로</span>
<span class="c"># explicit는 service를 on off를 하는 것을 triton에서 올린 이후 가능</span>
<span class="c"># 권장하는 control mode로 model을 CI/CD를 할때 적절하게 사용할 수 있는 옵션인것으로 생각된다.</span>
<span class="nt">--model-control-mode</span><span class="o">=</span>explicit
<span class="c"># curl로 service on</span>
curl <span class="nt">-X</span> POST localhost:8000/v2/repository/models/onnx_model/load
</code></pre></div></div>

<p>Triton에서는 들어오는 request queue를 처리하는 방법도 제어가 가능하다.</p>

<ol>
  <li>
    <p>Batch scheduler</p>

    <blockquote>
      <p>request의 batch 수에 따라서 가변적으로 변할 수 있게 하는 것으로 특정 size가 만족이 안되면 특정 시간이 지나면 바로 수행하도록 하는 것으로 해당 옵션은max_queue_delay_microseconds로 제어가 가능하다.</p>

    </blockquote>

    <p><img src="/assets/images/triton-handson/2022-07-22_07-12-56.png" alt="스크린샷, 2022-07-22 07-12-56.png" /></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># 아래 내용을 pdtxt 내용에 포함시키면 실행이 된다.</span>
 dynamic_batching <span class="o">{</span>
   preferred_batch_size: <span class="o">[</span> 4, 8, 16, 32 <span class="o">]</span>
   max_queue_delay_microseconds: 100
 <span class="o">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Multiple instances</p>

    <blockquote>
      <p>여러 개의 instance를 생성 시켜 병렬로 queue들을 처리하는 방법</p>

    </blockquote>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># 아래 내용을 pdtxt 내용에 포함시키면 실행이 된다.</span>
 instance_group <span class="o">[</span>
     <span class="o">{</span>
       count: 1
       kind: KIND_CPU
     <span class="o">}</span>
   <span class="o">]</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/triton-handson/2022-07-22_07-12-42.png" alt="스크린샷, 2022-07-22 07-12-42.png" /></p>
  </li>
</ol>

<ul>
  <li>
    <p>Triton client</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  docker run <span class="nt">-it</span> <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/hands_on <span class="nt">--gpus</span> <span class="s1">'"device=2"'</span> <span class="nt">--net</span><span class="o">=</span>host nvcr.io/nvidia/tritonserver:22.03-py3-sdk
</code></pre></div>    </div>

    <p>triton client는 도커 이름 뒤에 sdk가 붙는 것이 특징이다.</p>

    <ul>
      <li>
        <p>client에서 request 보내기</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="nn">tritonclient.http</span> <span class="k">as</span> <span class="n">tritonhttpclient</span>
  <span class="kn">import</span> <span class="nn">tritonclient.grpc</span> <span class="k">as</span> <span class="n">tritongrpcclient</span>
  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
  <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
  <span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
  <span class="kn">import</span> <span class="nn">json</span>
        
  <span class="c1">###CONFIGURATION########
</span>  <span class="n">VERBOSE</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">input_name</span> <span class="o">=</span> <span class="s">'actual_input_1'</span>
  <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
  <span class="n">input_dtype</span> <span class="o">=</span> <span class="s">'FP32'</span>
  <span class="n">output_name</span> <span class="o">=</span> <span class="s">'output_1'</span>
  <span class="n">model_name</span> <span class="o">=</span> <span class="s">'trt_model'</span>
  <span class="n">http_url</span> <span class="o">=</span> <span class="s">'localhost:8000'</span>
  <span class="n">grpc_url</span> <span class="o">=</span> <span class="s">'localhost:8001'</span>
  <span class="n">model_version</span> <span class="o">=</span> <span class="s">'1'</span>
  <span class="c1">########################
</span>        
  <span class="c1">#Image Loading
</span>  <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'./src/goldfish.jpg'</span><span class="p">)</span>
        
  <span class="n">imagenet_mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
  <span class="n">imagenet_std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
        
  <span class="n">resize</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
  <span class="n">center_crop</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
  <span class="n">to_tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()</span>
  <span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">imagenet_mean</span><span class="p">,</span>
                                   <span class="n">std</span><span class="o">=</span><span class="n">imagenet_std</span><span class="p">)</span>
        
  <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">resize</span><span class="p">,</span> <span class="n">center_crop</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">normalize</span><span class="p">])</span>
  <span class="n">image_tensor</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
        
  <span class="c1">#Label Loading
</span>        
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'./src/imagenet-simple-labels.json'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
        
  <span class="c1">#Start client set up
</span>        
  <span class="c1">#triton_client = tritonhttpclient.InferenceServerClient(url=http_url, verbose=VERBOSE)
</span>  <span class="n">triton_client</span> <span class="o">=</span> <span class="n">tritongrpcclient</span><span class="p">.</span><span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">grpc_url</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">VERBOSE</span><span class="p">)</span>
  <span class="n">model_metadata</span> <span class="o">=</span> <span class="n">triton_client</span><span class="p">.</span><span class="n">get_model_metadata</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="n">model_version</span><span class="p">)</span> <span class="c1">#You can remove this line
</span>  <span class="n">model_config</span> <span class="o">=</span> <span class="n">triton_client</span><span class="p">.</span><span class="n">get_model_config</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="n">model_version</span><span class="p">)</span>
        
  <span class="n">image_numpy</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="n">image_numpy</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        
  <span class="c1">#input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)
</span>  <span class="n">input0</span> <span class="o">=</span> <span class="n">tritongrpcclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span>
  <span class="c1">#input0.set_data_from_numpy(image_numpy, binary_data=False)
</span>  <span class="n">input0</span><span class="p">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">image_numpy</span><span class="p">)</span>
        
  <span class="c1">#output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)
</span>  <span class="n">output</span> <span class="o">=</span> <span class="n">tritongrpcclient</span><span class="p">.</span><span class="n">InferRequestedOutput</span><span class="p">(</span><span class="n">output_name</span><span class="p">)</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">triton_client</span><span class="p">.</span><span class="n">infer</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="n">model_version</span><span class="p">,</span> 
                                 <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input0</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">output_name</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        
  <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GitHub User</name></author><category term="triton" /><summary type="html"><![CDATA[Triton은 Nvidia에서 제공하는 Deploy open-source로 request, response로 제공하는 서비스에 대해 다양한 기능들을 제공한다.(model version 관리, service performance test, service batch scheduler and so on) 본 내용은 Nvidia 측에서 만들어주신 자료 기반으로 작성되었습니다. 해당 내용은 https://github.com/leejinho610/TRT_Triton_HandsOn 를 참고]]></summary></entry><entry><title type="html">나만의 Stable Diffusion 서비스 배포</title><link href="http://localhost:4000/bentoml/2022/10/03/Stable-Diffusion-bento.html" rel="alternate" type="text/html" title="나만의 Stable Diffusion 서비스 배포" /><published>2022-10-03T07:42:00-04:00</published><updated>2022-10-03T07:42:00-04:00</updated><id>http://localhost:4000/bentoml/2022/10/03/Stable-Diffusion-bento</id><content type="html" xml:base="http://localhost:4000/bentoml/2022/10/03/Stable-Diffusion-bento.html"><![CDATA[<p><a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a>은 <a href="http://stability.ai/">stability.ai</a>에서 출시한 text-to-image model 오픈소스 입니다. Stable Diffusion은 자연어 프롬프트로 부터 단 몇초만에 창의적인 예술품을 생성할 수 있습니다.</p>

<h2 id="stable-diffusion을-왜-online으로-불러오는-것인가">Stable Diffusion을 왜 Online으로 불러오는 것인가?</h2>

<p>제한적인 local 컴퓨터에서 Stable Diffusion 모델이 좋은 품질의 이미지를 생성하기 위해 오랜 시간이 필요합니다. 모델을 온라인 클라우드 서비스에서 실행하게 된다면 사실상 제한없는 컴퓨터 자원을 사용할 수 있게 되어 높은 품질의 결과물을 보다 빠르게 얻을 수 있습니다. Microservice 단위로 모델을 호스팅하면 <a href="https://modelserving.com/blog/why-do-people-say-its-so-hard-to-deploy-a-ml-model-to-production">ML 모델을 온라인에서 실행해야하는 복잡한 절차</a> 없이 모델 성능을 레버리지 할 수 있고 보다 창의적인 애플리케이션을 제작할 수 있습니다.</p>

<h2 id="stable-diffusion을-ec2에서-배포">Stable Diffusion을 EC2에서 배포</h2>

<p>Stable Diffusion 모델을 온라인으로 호스팅하는 한가지 방법은 BentoML 과 AWS의 EC2를 활용하는 것입니다. BentoML은 기계 학습 서비스를 대규모로 구축, 배포 및 운영할 수 있는 오픈소스 플랫폼입니다. 이 글에서는 BentoML을 사용하여 제품에 즉시 사용할 수 있는 Stable Diffusion 서비스를 생성하고 이를 AWS EC2에 배포합니다. 아래는 이 글의 절차들을 수행하면 얻을 수 있는 결과물 입니다.</p>

<p><img src="/assets/images/stable_diffusion/swagger.png" alt="swagger.png" /></p>

<p>RESTful OpenAPI 서비스 <code class="language-plaintext highlighter-rouge">/txt2img</code> (text to image) 와 <code class="language-plaintext highlighter-rouge">/img2img</code>(image + text to image) 앤드포인트들을 갖고 있는 Swagger 유저 인터페이스</p>

<p><img src="/assets/images/stable_diffusion/txt2img.png" alt="txt2img.png" /></p>

<p><code class="language-plaintext highlighter-rouge">/txt2img</code> 엔드포인트를 활용하여 텍스트 프롬프트로 생성된 예시 이미지</p>

<p><img src="/assets/images/stable_diffusion/img2img.png" alt="img2img.png" /></p>

<p><code class="language-plaintext highlighter-rouge">/img2img</code> 엔드포인트를 활용하여 이미지 및 텍스트 프롬프트로 생성된 예시 이미지</p>

<h2 id="prerequisites"><strong>Prerequisites</strong></h2>

<ul>
  <li>Python 3.9 or above</li>
  <li><a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">AWS CLI</a></li>
  <li><a href="https://docs.bentoml.org/en/latest/installation.html">BentoML</a></li>
  <li><a href="https://github.com/bentoml/bentoctl#installation">bentoctl</a></li>
  <li><a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">Terraform</a></li>
  <li><a href="http://docs.docker.com/install">Docker</a></li>
</ul>

<p>코드와 샘플들은 이 글(<em><a href="https://github.com/bentoml/stable-diffusion-bentoml">https://github.com/bentoml/stable-diffusion-bentoml</a></em>)에서 찾을 수 있습니다.</p>

<h2 id="환경-및-stable-diffusion-model-준비하기">환경 및 Stable Diffusion Model 준비하기</h2>

<p>저장소 복제 및 의존성 설치</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bentoml/stable-diffusion-bentoml.git <span class="o">&amp;&amp;</span> <span class="nb">cd </span>stable-diffusion-bentoml
python3 <span class="nt">-m</span> venv venv <span class="o">&amp;&amp;</span> <span class="nb">.</span> venv/bin/activate
pip <span class="nb">install</span> <span class="nt">-U</span> pip
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p>Stable Diffusion model을 선택하여 다운로드하세요. Single Precision(FP32)는 10GB 이상의 VRAM이 있는 CPU 또는 GPU에 적합합니다. Half Precision(FP16)는 10GB VRAM 미만의 GPU에 적합합니다.</p>

<p><strong>Single Precision (FP32)</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>fp32/
curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4.tgz | <span class="nb">tar </span>zxf - <span class="nt">-C</span> models/
</code></pre></div></div>

<p><strong>Half Precision (FP16)</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>fp16/
curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4_fp16.tgz | <span class="nb">tar </span>zxf - <span class="nt">-C</span> models/
</code></pre></div></div>

<h2 id="stable-diffusion-betno-구축">Stable Diffusion Betno 구축</h2>

<p>모델을 RESTful API로써 serve하기 위해 우리는 BentoML service를 만들것입니다. 다음 예제는 예측을 위한 single precision 모델과 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/fp32/service.py">service.py</a> 모듈을 사용하여 비즈니스 로직으로 서비스를 결합합니다. <code class="language-plaintext highlighter-rouge">@svc.api</code> 데코레이션을 활용하여 함수를 APIs로 노출시킬 수 있습니다. 뿐만아니라 input과 output의 type을 지정해줄 수 있습니다. 예를 들어, <code class="language-plaintext highlighter-rouge">txt2img</code> 앤드포인트는 <code class="language-plaintext highlighter-rouge">JSON</code> 을 입력을 받아 <code class="language-plaintext highlighter-rouge">Image</code> 출력을 반환하는 반면 <code class="language-plaintext highlighter-rouge">img2img</code> 앤드포인트는 <code class="language-plaintext highlighter-rouge">Image</code> 와 <code class="language-plaintext highlighter-rouge">JSON</code> 을 입력으로 받아 출력으로 <code class="language-plaintext highlighter-rouge">Image</code> 를 반환합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">svc</span><span class="p">.</span><span class="n">api</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">JSON</span><span class="p">(),</span> <span class="n">output</span><span class="o">=</span><span class="n">Image</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">txt2img</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stable_diffusion_runner</span><span class="p">.</span><span class="n">txt2img</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="o">@</span><span class="n">svc</span><span class="p">.</span><span class="n">api</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">Multipart</span><span class="p">(</span><span class="n">img</span><span class="o">=</span><span class="n">Image</span><span class="p">(),</span> <span class="n">data</span><span class="o">=</span><span class="n">JSON</span><span class="p">()),</span> <span class="n">output</span><span class="o">=</span><span class="n">Image</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">img2img</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stable_diffusion_runner</span><span class="p">.</span><span class="n">img2img</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p>inference logic의 핵심은 <code class="language-plaintext highlighter-rouge">StableDiffusionRunnable</code> 에 정의되어 있습니다. runnable은 모델에서 <code class="language-plaintext highlighter-rouge">txt2img_pipe</code> 및 <code class="language-plaintext highlighter-rouge">img2img_pipe</code> 메서드를 호출하고 필요한 arguments를 전달하는 역할을 합니다. custom runner는 API에서 모델 inference logic을 실행하기 위해 <code class="language-plaintext highlighter-rouge">StableDiffusionRunnable</code> 에서 인스턴스화 됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stable_diffusion_runner</span> <span class="o">=</span> <span class="n">bentoml</span><span class="p">.</span><span class="n">Runner</span><span class="p">(</span><span class="n">StableDiffusionRunnable</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'stable_diffusion_runner'</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<p>다음 명령어을 실행하여 테스트용 BentoML 서비스를 시작합니다. 로컬의 CPU에서 Stable Diffusion 모델 추론을 실행하는 것은 매우 느립니다. 각 요청을 처리하는데 약 5분이 소요됩니다. 다음 섹션에서는 GPU가 있는 머신에서 서비스를 실행하여 추론 속도를 가속화 하는 방법을 탐구할 것입니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">BENTO_CONFIG</span><span class="o">=</span>configuration.yaml bentoml serve service:svc <span class="nt">--production</span>
</code></pre></div></div>

<p>Curl the text-to-image <code class="language-plaintext highlighter-rouge">/txt2img</code>endpoint.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://127.0.0.1:3000/txt2img <span class="nt">-H</span> <span class="s1">'Content-Type: application/json'</span> <span class="nt">-d</span> <span class="s2">"{</span><span class="se">\"</span><span class="s2">prompt</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">View of a cyberpunk city</span><span class="se">\"</span><span class="s2">}"</span> <span class="nt">--output</span> output.jpg
</code></pre></div></div>

<p>Curl the image-to-image /img2img endpoint.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://127.0.0.1:3000/img2img <span class="nt">-H</span> <span class="s1">'Content-Type: multipart/form-data'</span> <span class="nt">-F</span> <span class="nv">img</span><span class="o">=</span><span class="s2">"@input.jpg"</span> <span class="nt">-F</span> <span class="nv">data</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">prompt</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">View of a cyberpunk city</span><span class="se">\"</span><span class="s2">}"</span> <span class="nt">--output</span> output.jpg
</code></pre></div></div>

<p>필요 파일 및 종속성은 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/fp32/bentofile.yaml">bentoml.yaml</a> 파일에 정의되어 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">service</span><span class="pi">:</span> <span class="s2">"</span><span class="s">service.py:svc"</span>
<span class="na">include</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">service.py"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">requirements.txt"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">models/v1_4"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">configuration.yaml"</span>
<span class="na">python</span><span class="pi">:</span>
  <span class="na">packages</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">torch</span>
    <span class="pi">-</span> <span class="s">transformers</span>
    <span class="pi">-</span> <span class="s">diffusers</span>
    <span class="pi">-</span> <span class="s">ftfy</span>
<span class="na">docker</span><span class="pi">:</span>
  <span class="na">distro</span><span class="pi">:</span> <span class="s">debian</span>
  <span class="na">cuda_version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">11.6.2"</span>
  <span class="na">env</span><span class="pi">:</span>
    <span class="na">BENTOML_CONFIG</span><span class="pi">:</span> <span class="s2">"</span><span class="s">src/configuration.yaml"</span>
</code></pre></div></div>

<p>아래 명령어로 Bento를 만들 수 있습니다. Bento는 BentoML 서비스의 배포 형식입니다. 서비스 실행에 필요한 파일과 설정들을 포함하는 독립적 아카이브 입니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoml build
</code></pre></div></div>

<p>🎉Stable Diffusion bento가 구축되었습니다. 어떠 이유로 bento를 성공적으로 만들 수 없었다면 걱정하지 마세요 아래 명령어를 이용하여 사전 제작된 bento를 다운로드 할 수 있습니다.</p>

<p><strong>Download Single Precision (FP32) Stable Diffusion Bento</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_fp32.bento <span class="o">&amp;&amp;</span> bentoml import ./sd_fp32.bento
</code></pre></div></div>

<p><strong>Download Half Precision (FP16) Stable Diffusion Bento</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_fp16.bento <span class="o">&amp;&amp;</span> bentoml import ./sd_fp16.bento
</code></pre></div></div>

<h2 id="ec2에서-stable-diffusion-bento-배포">EC2에서 Stable Diffusion Bento 배포</h2>

<p>우리는 <a href="https://github.com/bentoml/bentoctl">bentoctl</a>을 이용하여 bento를 EC2에 배포할 것입니다. <code class="language-plaintext highlighter-rouge">bentoctl</code>는 당신의 bento들을 Terraform로 클라우드 플랫폼에 배포하는 것을 돕습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl operator <span class="nb">install </span>aws-ec2
</code></pre></div></div>

<p>배포를 위한 설정들이 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/bentoctl/deployment_config.yaml">deployment_config.yaml</a> 파일에 구성되어 있습니다. 해당 사양들을 자유롭게 업데이트 해주세요. 기본 설정으로 <code class="language-plaintext highlighter-rouge">us-west-1</code> region에 <em>Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) AMI</em>가 있는 <a href="https://aws.amazon.com/ec2/instance-types/g4/">g4dn.xlarge</a> 인스턴스에 Bento가 배포하도록 구성되어 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">api_version</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">stable-diffusion-demo</span>
<span class="na">operator</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">aws-ec2</span>
<span class="na">template</span><span class="pi">:</span> <span class="s">terraform</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">region</span><span class="pi">:</span> <span class="s">us-west-1</span>
  <span class="na">instance_type</span><span class="pi">:</span> <span class="s">g4dn.2xlarge</span>
  <span class="c1"># points to Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) 20220913 AMI</span>
  <span class="na">ami_id</span><span class="pi">:</span> <span class="s">ami-0a85a3a3fb34b3c7f</span>
  <span class="na">enable_gpus</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p>Terraform 파일 생성</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl generate <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<p>Docker 이미지를 만들고 AWS ECR로 push 합니다. 이미지 업로드는 대역폭에 따라 시간이 오래 걸릴 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl build <span class="nt">-b</span> stable_diffusion_fp32:latest
</code></pre></div></div>

<p>AWS EC2에 bento를 배포하기 위해 Terraform 파일을 등록합니다. EC2 콘솔에서 브라우저를 퍼블릭 IP 주소로 공개하여 Swagger UI에 접근이 가능합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl apply <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<p>마지막으로 Stable Diffusion BentoML 서비스가 더 이상 필요없다면 배포를 삭제합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl destroy <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<h1 id="결론">결론</h1>

<p>이 글에서 저희는 BentoML을 사용하여 Stable Diffusion을 위한 production-ready 서비스를 구축하고 AWS EC2에 배포했습니다. AWS EC2에 서비스를 배포함으로써 더 강력한 하드웨어에서 Stable Diffusion 모델을 짧은 지연시간으로 이미지를 생성하고 단일 시스템 이상으로 확장할 수 있었습니다. 이 글을 재미있게 읽었다면 <a href="https://github.com/bentoml/BentoML">github의 Bentoml project</a>에 ⭐ 와 <a href="https://l.bentoml.com/join-slack">slack community</a>에서 마음에 맞는 분들을 만나보시길 바랍니다.</p>

<h1 id="reference">Reference</h1>
<p>해당 자료는 BentoML Blog를 번역한 자료 입니다.
<a href="https://modelserving.com/blog/deploying-your-own-stable-diffusion-service-mz9wk">원본 자료</a></p>]]></content><author><name>GitHub User</name></author><category term="BentoML" /><summary type="html"><![CDATA[Stable Diffusion은 stability.ai에서 출시한 text-to-image model 오픈소스 입니다. Stable Diffusion은 자연어 프롬프트로 부터 단 몇초만에 창의적인 예술품을 생성할 수 있습니다.]]></summary></entry></feed>