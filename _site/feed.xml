<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.2">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2022-11-19T12:27:54-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Lomit tech blog</title><subtitle>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.</subtitle><author><name>GitHub User</name></author><entry><title type="html">Terminal의 마술사 tmux</title><link href="http://localhost:4000/utils/2022/10/30/tmux.html" rel="alternate" type="text/html" title="Terminal의 마술사 tmux" /><published>2022-10-30T05:40:00-04:00</published><updated>2022-10-30T05:40:00-04:00</updated><id>http://localhost:4000/utils/2022/10/30/tmux</id><content type="html" xml:base="http://localhost:4000/utils/2022/10/30/tmux.html"><![CDATA[<h2 id="what-is-tmux">what is tmux??</h2>

<p>SSH 접속하여 프로그램을 구동시키는 황경에서 ssh 접속이 끊어지면 프로그램이 같이 종료되는 것을 경험 해보았을 것이다. 이러한 현상의 대처법으로 nohup이라는 것을 이용하여 해당 프로그램을 백그라운드로 구동시키는 방법이 있지만 tmux를 이용하여 세션을 따로 할당하여 프로그램을 구동시키는 방법도 있다. 이외에 tmux에서 제공하는 멀티윈도우 기능이 터미널 작업에서 유용하게 사용하니 같이 공부해보자</p>

<h2 id="tmux-session-생성">tmux session 생성</h2>

<p>tmux에는 session이라는 것이 있다. 세션은 작업들을 하나로 묶은 개념이다. 예제를 보면서 진행한다면 조금 더 이해가 편할 것이다.</p>

<p>터미널 상에 <code class="language-plaintext highlighter-rouge">tmux</code> 를 입력하게 되면 세션이 하나가 할당되면서 터미널 창이 하나가 나오면서 아래와 같이 밑에 초록색으로  상태를 보여주는 상태 표시줄이 나온다.</p>

<p><img src="/assets/images/tmux/2021-03-03_16-03-49.png" alt="sample" /></p>

<p>tmux을 아무 옵션없이 세션을 생성한다면 세션 이름이 0으로 생성이되면서 다음에 생성되는 세션의 이름은 1으로 생성이 된다.</p>

<p>이렇게 작업을 하는 것은 작업에 있어 혼동을 줄 수 있으니 <strong>세션에 이름을 부여하는 방법</strong>을 배워보자</p>

<p>아래와 같이 옵션을 주게 되면 세션 이름으로 세션이 생성이 된다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux new <span class="nt">-s</span> work_1
</code></pre></div></div>

<p><img src="/assets/images/tmux/2021-03-03_16-09-28.png" alt="/assets/images/tmux/_2021-03-03_16-09-28.png" /></p>

<p>(아래의 상태창을 보면 세션의 이름을 확인할 수 있다.)</p>

<h2 id="개발환경-실습">개발환경 실습</h2>

<p>tmux에서는 명령모드라는 것이 있다. 세션에서 여러개 화면이나 세션을 관리하는 작업을 위해서 사용하는 것으로 명령모드에 들어가려면 <code class="language-plaintext highlighter-rouge">ctrl + b</code> 을 누르면 명령모드로 들어갈 수 있다.</p>

<p><strong>명령모드(ctrl + b)에서</strong> <code class="language-plaintext highlighter-rouge">"</code> 를 입력하면  화면 처럼 하나의 윈도우가 생기면서 각기 터미널이 활성화가 된다.</p>

<p><img src="/assets/images/tmux/2021-03-03_16-43-38.png" alt="/assets/images/tmux/_2021-03-03_16-43-38.png" /></p>

<p><strong>명령모드(ctrl + b)에서 %</strong>를 입력하면 오른쪽으로 윈도우가 생성된다.</p>

<p><img src="/assets/images/tmux/2021-03-03_16-44-14.png" alt="/assets/images/tmux/_2021-03-03_16-44-14.png" /></p>

<p><strong>명령모드(ctrl + b)에서 방향키</strong>를 누르면 사용하는 화면을 이동할 수 있다.</p>

<p><img src="/assets/images/tmux/2021_03_03_16_44_55.gif" alt="sample" /></p>

<p><strong>명령모드(ctrl + b)에서 z</strong>를 입력하면 해당 윈도우가 큰 화면으로 볼 수 있다. 다시 같은 작업을 진행하게 되면 원래 상태로 돌아온다.</p>

<p><img src="/assets/images/tmux/2021_03_03_16_47_02.gif" alt="s" /></p>

<p><strong>명령모드(ctrl + b)에서 alt키와 방향키</strong>를 이용하여 윈도우의 크기를 조절할 수 있다.</p>

<p><img src="/assets/images/tmux/2021_03_03_16_48_42.gif" alt="/assets/images/tmux/_2021_03_03_16_48_42.gif" /></p>

<p><strong>명령모드(ctrl + b)에서 d</strong>키를 입력하면 해당 세션에서 나올 수 있다.(종료되는 것이 아님)</p>

<p><img src="/assets/images/tmux/2021_03_03_16_49_36.gif" alt="/assets/images/tmux/_2021_03_03_16_49_36.gif" /></p>

<p>세션에서 나와도 <code class="language-plaintext highlighter-rouge">tmux ls</code>로 실행 중인 세션을 확인할 수 있다.</p>

<p><img src="/assets/images/tmux/2021-03-03_16-51-06.png" alt="/assets/images/tmux/_2021-03-03_16-51-06.png" /></p>

<h2 id="종료한-세션으로-다시-어떻게-들어갈-수-있을까">종료한 세션으로 다시 어떻게 들어갈 수 있을까??</h2>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>tmux at <span class="nt">-t</span> &lt;session-name&gt;
</code></pre></div></div>

<p><img src="/assets/images/tmux/2021_03_03_16_52_00.gif" alt="/assets/images/tmux/_2021_03_03_16_52_00.gif" /></p>]]></content><author><name>GitHub User</name></author><category term="Utils" /><summary type="html"><![CDATA[what is tmux??]]></summary></entry><entry><title type="html">MLops is all you need?</title><link href="http://localhost:4000/mlops/2022/10/25/MLops-is-all-you-need.html" rel="alternate" type="text/html" title="MLops is all you need?" /><published>2022-10-25T05:40:00-04:00</published><updated>2022-10-25T05:40:00-04:00</updated><id>http://localhost:4000/mlops/2022/10/25/MLops-is-all-you-need</id><content type="html" xml:base="http://localhost:4000/mlops/2022/10/25/MLops-is-all-you-need.html"><![CDATA[<h1 id="what-is-mlops">What is MLOps??</h1>
<p>⚠️  필자의 개인적인 의견이 많은 내용으로 유의바람</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled.png" alt="MLOps = ML + Dev + Ops" />
MLOps = ML + Dev + Ops</p>

<p>많은 사람들이 MLops에 대한 이해를 ML + Devops라고 정의한다. 이보다 명확한 설명이 Wikipedia에 있다.</p>

<blockquote>
  <p><strong>MLOps</strong> or <strong>ML Ops</strong> is a set of practices that aims to <strong>deploy and maintain</strong> <a href="https://en.wikipedia.org/wiki/Machine_learning">machine learning</a> models in production reliably and efficiently.</p>

</blockquote>

<p>해석하자면 MLOps는 머신 러닝 모델을 안정적이고 효율적으로 배치하고 유지하는 것을 목표로 하는 일련의 관행이다. 여기서 중요하게 생각해야하는 것은 <strong>안정적, 효율적, 배치, 유지</strong> 이다.</p>

<h2 id="why-to-study-mlops">Why to study MLOps??</h2>
<h3 id="ml-competition">ML competition</h3>

<p>필자는 kaggle과 dacon, 머신러닝 경진대회를 통해서 모델링과 ML문제들을 풀었다.</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%201.png" alt="kaggle" /></p>

<!-- kaggle -->

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%202.png" alt="dacon" /></p>

<!-- dacon -->

<p>경진대회를 통해 여러 공부들을 많이 했지만 <strong>“real world에서 발생하는 문제들을 내가 직접 해결할 수 있을까??”</strong></p>

<p>에 대해서 고민하였을 때 스스로에게 부족함을 많이 느꼈다. 그 이유는 경진대회에서는 이미 준비가 되어 있는 것이 많다. 그 예로 두 가지가 있는데</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%203.png" alt="Untitled" /></p>

<ul>
  <li>데이터셋 (물론 train / test 데이터 셋을 분류해서 제공해줌)</li>
  <li>정의되어 있는 metrics</li>
</ul>

<p>Test 데이터 셋과 metrics는 프로젝트의 이정표와 같다. 제공해야하는 서비스의 방향성과 설정한 Test 데이터 셋과 metrics가 설정된다면 프로젝트 기간과 resource들을 낭비가 일어날 수 밖에 없다.</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%204.png" alt="이정표" /></p>

<!-- 이정표 -->

<h3 id="ml-서비스-개발을-위한-절차">ML 서비스 개발을 위한 절차</h3>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%205.png" alt="데이터 확보 부터 배포까지" /></p>

<!-- 데이터 확보 부터 배포까지 -->

<p>ML 서비스가 나오기 위해서는 복잡한 절차를 통해서 나오는 것이지만 간단하게 표현하자면 <strong>데이터 수집 → 데이터 처리 → 모델 개발 → 서비스 배포</strong> 절차로 진행된다.(기획, 서비스 관리 등 복잡하고 어려운 것들이 많다.) 많은 ML competition 플랫폼들과 많은 ML 개발자, 학생들도 모델 개발에 많은 집중을 한다. 하지만 이 부분은 ML 서비스를 만들기 위해 극히 일부분에 해당한다.</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%206.png" alt="UC Berkely 졸업생 대상 설문 조사 결과" /></p>

<!-- UC Berkely 졸업생 대상 설문 조사 결과 -->

<p>위는 트위터를 통해 UC Berkely 졸업생 대상으로 ML service를 만들때 어렵다고 생각하는 Task가 무엇인지 설문한 것이다. 졸업생들은 <strong>Model training</strong>은 어렵지 않게 진행할 수 있다고 답변하였으며 나머지에 대해서는 대부분 어렵다고 답변하였다. 특히 <strong>Model monitoring, Model maintenance and retraing</strong>은 매우 어렵다고 답변하고 있다. 그 이유를 생각해보자면 해당 Task로 구성되어 있는 수업이 흔하지 않다는 것이 가장 큰 이유일 것 같다.</p>

<p>(이중 설문 결과가 특이한 Task가 Data labeling이다. 다른 Task에 비해 결과의 분산량이 크다. 이는 설문자 별로 생각하는 Data labeling이 서로 달라 발생하는 결과인 것 같다. 예를 들자면 단순한 노동으로 생각하는 설문자도 있으며 라벨링 또한 어렵고 복잡한 Task라고 평가하는 설문자들도 있는 것으로 생각된다.)</p>

<h2 id="how-to-build-mlops">How to build MLops</h2>

<h3 id="google에서의-ml-service">Google에서의 ML service</h3>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%207.png" alt="**[Hidden Technical Debt in Machine Learning Systems](https://papers.neurips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf)(google) 2015**" /></p>

<p><strong><a href="https://papers.neurips.cc/paper/5656-hidden-technical-debt-in-machine-learning-systems.pdf">Hidden Technical Debt in Machine Learning Systems</a>(google) 2015</strong></p>

<p>구글에서는 ML service를 만들때 ML code는 전체 시스템에서 가장 작은 부분을 차지한다고 평가하고 있다. 심지어 구글에서 발표한 논문 제목을 머신러닝을 숨겨진 부체라고 표현하고 있다. 논문 내용에서는 Machine learning system은 데이터 종속적인 점을 중심으로 다양한 문제점들을 기술하고 있다. 그럴만한게 머신러닝 모델은 데이터로 부터 생성되는 부산물로써 좋지 않은 데이터에서는 좋지 않는 머신러닝 모델이 나오는 것은 당연하다.</p>

<h3 id="data-dependencies-system">Data dependencies system</h3>

<p>Machine learning으로 서비스를 제공하기 이전과 이후를 비교한다면 아래와 같다.</p>

<ul>
  <li>ML system이 있기 이전에는 어떤 것이 필요했을까? (Software 1.0)
    <ul>
      <li>computing</li>
      <li>code + algorithm</li>
    </ul>
  </li>
  <li>ML system 도입을 위해 필요한 것은? (Software 2.0)
    <ul>
      <li>computing</li>
      <li>code + algorithm = model</li>
      <li><strong>Data</strong></li>
    </ul>
  </li>
</ul>

<p>Software 1.0과 Software 2.0의 차이는 Data라고 볼 수 있다. 물론 Software 1.0에서도 Data를 활용하였지만 Software 2.0에서 데이터를 활용하여 모델을 만들거나 알고리즘을 만들만큼 중요한 요소로 적용되지 않았을 것으로 예상된다. 이에 Software2.0으로 넘어오면서 부족한 요소일 것이며 중요한 요소가 될 것으로 예상된다.</p>

<h3 id="real-world-ml-service-데이터의-기여도">real-world ML service 데이터의 기여도</h3>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%208.png" alt="안드레이 카파시(테슬라 AI 해더)의 잃어버린 잠의 양" /></p>

<!-- 안드레이 카파시(테슬라 AI 해더)의 잃어버린 잠의 양 -->

<p>안드레이가 박사시절과 테슬라 시절에 잠을 빼앗긴 양을 표현한 차트입니다. 박사시절에는 모델과 알고리즘을 손봐야하는 일이 많았지만 이에 반해 실무에서는 데이터를 손봐야하는 일이 많아졌다고 한다.</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%209.png" alt="Data quality vs Model quality (Andrew ng)" /></p>

<!-- Data quality vs Model quality (Andrew ng) -->

<p>데이터 품질을 올리면 어떤 효과를 볼 수 있는지 나타낸 이미지로 model로 성능을 증진시키는 것 보다 더 많은 증진을 이뤄낸 것을 볼 수 있다.</p>

<p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2010.png" alt="Data-Centric AI competition(Andrew ng)" /></p>

<!-- Data-Centric AI competition(Andrew ng) -->

<p>이에 앤드류 응 교수님은 기존의 model centric competition이 아닌 data centric comeptition을 계최하였다.</p>

<h3 id="data-handling-of-tesla">Data handling of Tesla</h3>

<p>데이터를 활용하여 극적인 성능으로 머신러닝 서비스를 제공하고 있는 기업으로 테슬라가 있다고 생각한다. 그렇다면 테슬라에서는 어떤 방법으로 Data centric ML service를 제공하는지 알아보자</p>

<ul>
  <li>
    <p>bicycle object detection</p>

    <p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2011.png" alt="(figure 1.) 서비스 제공 이전 테슬라의 데이터 샘플" /></p>

    <p>(figure 1.) 서비스 제공 이전 테슬라의 데이터 샘플</p>

    <p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2012.png" alt="(figure 2.)데이터 샘플 라벨링 결과물" /></p>

    <p>(figure 2.)데이터 샘플 라벨링 결과물</p>

    <p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2013.png" alt="(figure 3.)서비스 중 발생한 데이터 추론 결과" /></p>

    <p>(figure 3.)서비스 중 발생한 데이터 추론 결과</p>

    <p>일반적인 상황의 데이터들은 (figure 1.)과 같았을 것이고 데이터 라벨링 또한 (figure 2.)과 같이 진행했을 것이다. 하지만 만약 (figure 3.)과 같은 데이터가 들어오게 된다면 어떻게 될까?? 자율주행 차량의 알고리즘은 서로 다른 객체로 인식하여 차량 앞에는 자전거가 있어 급정거 혹은 브레이크 이상작동으로 서비스의 품질이 떨어져 이를 해결하기 위해 fleet learning을 고안해냈다.</p>
  </li>
  <li>
    <p>fleet learning</p>

    <p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2014.png" alt="(figure 4.) 서비스 중에 발생하는 데이터" />
  (figure 4.) 서비스 중에 발생하는 데이터</p>

    <p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2015.png" alt="(figure 5.) 넓은 영역에서 데이터가 발생" />
  (figure 5.) 넓은 영역에서 데이터가 발생</p>

    <p><img src="/assets/images/MLOps-is-all-you-need/Untitled%2016.png" alt="(figure 6.) fleet learning architecture " />
  (figure 6.) fleet learning architecture</p>

    <p>테슬라는 자전거로 의해 발생하는 브레이크 오작동을 해결하기 위해 실제 서비스중에 발생하는 데이터를 수집하여 관리하여 이를 해결하였다.</p>

    <p>fleet learning에 대해서 간단하게 알아보자면 아래와 같은 순서로 작동한다.(figure 6.)</p>

    <ol>
      <li>Data source(테슬라 사용자들의 차량들을 지칭)에서 부정확한 결과를 trigger로 전달
        <ul>
          <li>예를 들면 자율주행 모드를 사용자가 껐다.</li>
        </ul>
      </li>
      <li>unit test로 실제 찾고자하는 경우의 데이터인지 판단
        <ul>
          <li>unit test를 예를 들자면 위와 같이 자전거와 차량을 같은 객체라고 인식하지 않는 경우를 차량의 bounding box와 자전거의 bounding box가 서로 overlap되어 있는 상황(필자의 추측)</li>
        </ul>
      </li>
      <li>찾고자하는 경우인 데이터라고 하면 Boost를 통해 사용자에게 데이터 사용허가를 받아 데이터 셋을 수집 및 라벨링을 진행</li>
      <li>추가된 데이터셋을 활용하여 다시 재학습</li>
      <li>재학습된 모델을 Data source에 deploy</li>
    </ol>

    <p>Fleet learning으로 생성되는 데이터는 Large Dataset(figure 5.), Varied dataset(figure 4.), Real dataset(figure 4.) 특징이 있어 서비스에 적합한 데이터셋을 얻을 수 있다.</p>

    <ul>
      <li>Large dataset</li>
      <li>Varied dataset</li>
      <li>Real dataset</li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<p><a href="https://karpathy.medium.com/software-2-0-a64152b37c35">Software 2.0</a></p>

<p><a href="https://www.youtube.com/watch?v=FnFksQo-yEY&amp;t=1069s">How AI Powers Self-Driving Tesla with Elon Musk and Andrej Karpathy</a></p>

<p><a href="https://www.youtube.com/watch?v=06-AZXmwHjo&amp;t=419s">A Chat with Andrew on MLOps: From Model-centric to Data-centric AI</a></p>

<h2 id="closing-thought">Closing Thought</h2>

<h3 id="ml-system-도입을-위해-필요한-것은-software-20">ML system 도입을 위해 필요한 것은? (Software 2.0)</h3>

<ul>
  <li>computing</li>
  <li>code + algorithm = model</li>
  <li><strong>Data</strong></li>
  <li>(필자가 중요하게 생각하는 것)
    <ul>
      <li>Issue tracking</li>
      <li>data drift</li>
      <li>model uncertainty</li>
    </ul>
  </li>
</ul>

<h3 id="필자의-궁금증">필자의 궁금증</h3>

<ul>
  <li>
    <p>과연 MLOps는 ML로 무언가를 만드는 사람들이라면 모두 필요한 것인가?? (MLOps is all you need??)</p>

    <p>기업에서 MLOps 도입을 위해서는 전사적 협조가 필요해 보인다. 그리하여 작은 비용으로 쉽게 적용할 수 있는 것은 아니라고 보인다.</p>

    <p>모두가 MLOps를 도입해야하는 것이냐고 생각해보면 그런것 같지는 않은 것 같다. 이를 분류할 수 있는 기준은 ML 모델이 얼마나 자주 기업에서 제공하는 서비스에 관여하냐에 따라 나눠질 것 같다. 조금 더 쉽게 표현하자면 ML 모델이 반복적으로 inference를 하냐 하지않냐에 따라 분류될 수 있을 것 같다.</p>
  </li>
  <li>
    <p>decision cost는??</p>

    <p>ML 서비스를 안정적으로 배포하고 유지하려면 많은 비용들이 발생할 것으로 보인다. 그로 인해 기업에서는 ML 서비스로 도입하였을 때 얻을 수 있는 가치와 불안정성에 의한 Cost는 계산이 필요해 보인다.</p>

    <p>어떠한 Task에 어떤 Decision을 내리냐에 따라서 부과되는 cost는 매우 다를 것이라고 생각된다. <strong>얼마나 가치 있는 일</strong>을 혹은 <strong>반복적인 일</strong>인지에 따라서 얻을 수 있는 가치가 평가가 필요하며 잘못된 Decision으로 발생하는 비용는 ML 서비스 기획 부터 운용관리 때까지 지속적으로 평가해야할 것으로 보여 ML 모델에 대한 모니터링이 필요해 보인다.</p>
  </li>
</ul>]]></content><author><name>GitHub User</name></author><category term="MLops" /><summary type="html"><![CDATA[What is MLOps?? ⚠️ 필자의 개인적인 의견이 많은 내용으로 유의바람]]></summary></entry><entry><title type="html">Triton hands-on</title><link href="http://localhost:4000/triton/2022/10/04/Triton-hands-on.html" rel="alternate" type="text/html" title="Triton hands-on" /><published>2022-10-04T05:40:00-04:00</published><updated>2022-10-04T05:40:00-04:00</updated><id>http://localhost:4000/triton/2022/10/04/Triton-hands-on</id><content type="html" xml:base="http://localhost:4000/triton/2022/10/04/Triton-hands-on.html"><![CDATA[<blockquote>
  <p>Triton은 Nvidia에서 제공하는 Deploy open-source로 request, response로 제공하는 서비스에 대해 다양한 기능들을 제공한다.(model version 관리, service performance test, service batch scheduler and so on) 본 내용은 Nvidia 측에서 만들어주신 자료 기반으로 작성되었습니다. 해당 내용은 https://github.com/leejinho610/TRT_Triton_HandsOn 를 참고</p>

</blockquote>

<h2 id="1-pytorch-model-convert">1. PyTorch model convert</h2>

<blockquote>
  <p>model을 deploy 시 model file framework가 다양하다. 이번 hands-on에서는 PyTorch 관련한 framework를 다룰 예정이다.(torch-script, Onnx, tensorRT)</p>

  <ul>
    <li>base model</li>
  </ul>
</blockquote>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>hands-on간에 사용하고자 하는 base model 이다.

```python
from torchvision import models

model = models.wide_resnet101_2(pretrained=True).eval().cuda()
```
</code></pre></div></div>

<ul>
  <li>
    <p>Torch-script</p>

    <p>torch-script는 pytorch model을 보다 가속화하기 위한 framework로 C++에서도 사용이 가능하다. 보다 자세한 내용은 <a href="https://pytorch.org/docs/stable/jit.html">pytorch 공식 documentation</a>을 참고하는 것이 좋을 것 같다.</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">script_model</span> <span class="o">=</span> <span class="n">torch</span><span class="p">.</span><span class="n">jit</span><span class="p">.</span><span class="n">script</span><span class="p">(</span><span class="n">model</span><span class="p">)</span>
  <span class="n">script_model</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'model.pt'</span><span class="p">)</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Onnx</p>

    <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="n">input_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"actual_input_1"</span><span class="p">]</span>
  <span class="n">output_names</span> <span class="o">=</span> <span class="p">[</span><span class="s">"output_1"</span><span class="p">]</span>
  <span class="n">torch</span><span class="p">.</span><span class="n">onnx</span><span class="p">.</span><span class="n">export</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">torch</span><span class="p">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">).</span><span class="n">cuda</span><span class="p">(),</span> <span class="s">'model.onnx'</span><span class="p">,</span>
                    <span class="n">input_names</span><span class="o">=</span><span class="n">input_names</span><span class="p">,</span> <span class="n">output_names</span><span class="o">=</span><span class="n">output_names</span><span class="p">,</span>
                    <span class="n">dynamic_axes</span><span class="o">=</span><span class="p">{</span><span class="s">'actual_input_1'</span><span class="p">:{</span><span class="mi">0</span><span class="p">:</span><span class="s">'batch_size'</span><span class="p">},</span> <span class="s">'output_1'</span><span class="p">:</span> <span class="p">{</span><span class="mi">0</span><span class="p">:</span><span class="s">'batch_size'</span><span class="p">}})</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>tensorRT</p>

    <p>tensorRT는 Nvidia GPU에 최적화하는 툴로 방법이 크게 두 가지가 있다. (1. trtexec 활용 2. torchtrt 활용)</p>

    <p>📢 주의!! tensorRT는 target이 되는 GPU에서 진행해야 한다.</p>

    <p>나쁜 예) tensorRT 변환은 2080TI에서 진행하고 변환한 파일을 3080TI에서 deploy</p>

    <ol>
      <li>
        <p>trtexec</p>

        <blockquote>
          <p>필자가 직접 환경을 설치하여 tensorRT 변환하는 환경을 만들어 봤지만 많이 어려웠으며 결국 실패하였다. 오기로 시작하였지만 무수한 애러를 보고 컴퓨터에게 졌다… 변환하는 것은 docker를 사용하는 것을 추천하며 아래 docker command를 실행하면 쉽게 변환이 가능하다.</p>

        </blockquote>

        <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> docker run <span class="nt">--gpus</span> <span class="s1">'"device=0"'</span> <span class="nt">-it</span> <span class="nt">--rm</span> <span class="nt">-p</span> 8887:8887 <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/hands_on <span class="o">[</span>nvcr.io/nvidia/pytorch:22.03-py3]<span class="o">(</span>http://nvcr.io/nvidia/pytorch:22.03-py3<span class="o">)</span>
</code></pre></div>        </div>

        <ul>
          <li>TensorRT 변환
            <ul>
              <li>
                <p>using trtexec</p>

                <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  trtexec <span class="se">\</span>
    <span class="nt">--onnx</span><span class="o">=</span>model.onnx <span class="se">\</span>
    <span class="nt">--explicitBatch</span> <span class="se">\</span>
    <span class="nt">--optShapes</span><span class="o">=</span>actual_input_1:16x3x224x224 <span class="se">\</span>
    <span class="nt">--maxShapes</span><span class="o">=</span>actual_input_1:32x3x224x224 <span class="se">\</span>
    <span class="nt">--minShapes</span><span class="o">=</span>actual_input_1:1x3x224x224 <span class="se">\</span>
    <span class="nt">--best</span> <span class="se">\</span>
    <span class="nt">--saveEngine</span><span class="o">=</span>model.plan <span class="se">\</span>
    <span class="nt">--workspace</span><span class="o">=</span>4096
</code></pre></div>                </div>

                <ul>
                  <li>argument description
                    <ul>
                      <li>onnx : model file의 path</li>
                      <li>explicitBatch</li>
                      <li>optShapes : 최적화하고자 하는 batch shape</li>
                      <li>maxShapes : 최대 batch shape</li>
                      <li>minShapes : 최소 batch shape</li>
                      <li>best : network의 precision을 결정하는 argument로 default가 best를 활성화한 것이며 –fp16, –int8, –noTF32 와 같다.</li>
                      <li>saveEngine : tensorRT 변환 후 저장하고자 하는 이름</li>
                      <li>workspace : 최적화를 위하여 할당하고자 하는 memory size이며 이후 버전 부터 삭제예정</li>
                    </ul>
                  </li>
                </ul>

                <hr />

                <ul>
                  <li>
                    <p>trtexec dummy test</p>

                    <blockquote>
                      <p>변환 이후 변환한 tensorRT 파일이 어느정도 성능이 나오는지 한줄 코드로 수행이 가능하다.</p>

                    </blockquote>

                    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c"># 1. dummy input으로 inference 시간 측정</span>
  trtexec <span class="nt">--loadEngine</span><span class="o">=</span>model.plan <span class="nt">--dumpOutput</span>
                    
  <span class="c"># 2. dummy input으로 layer 별 시간 측정</span>
  trtexec <span class="nt">--loadEngine</span><span class="o">=</span>model.plan <span class="nt">--dumpProfile</span>
</code></pre></div>                    </div>
                  </li>
                </ul>
              </li>
              <li>
                <p>torchtrt libray</p>

                <p>python의 library로 제공되는 방법으로 쉽게 적용할 수 있지만 변환 후 performance는 trtexec 보다 떨어지는 것으로 알려져있다.</p>

                <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="c1">#Optional
</span>  <span class="kn">import</span> <span class="nn">torch_tensorrt</span> <span class="k">as</span> <span class="n">torchtrt</span>
                
  <span class="n">model</span> <span class="o">=</span> <span class="n">models</span><span class="p">.</span><span class="n">wide_resnet101_2</span><span class="p">(</span><span class="n">pretrained</span><span class="o">=</span><span class="bp">True</span><span class="p">).</span><span class="nb">eval</span><span class="p">().</span><span class="n">cuda</span><span class="p">()</span>
  <span class="c1">#Or you can load torchscript file directly likes
</span>  <span class="c1">#model = torch.jit.load('ts_model_path')
</span>                
  <span class="n">trt_module</span> <span class="o">=</span> <span class="n">torchtrt</span><span class="p">.</span><span class="nb">compile</span><span class="p">(</span><span class="n">model</span><span class="p">,</span> <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">torchtrt</span><span class="p">.</span><span class="n">Input</span><span class="p">(</span>
                                  <span class="n">min_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
                                  <span class="n">opt_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">16</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span>
                                  <span class="n">max_shape</span><span class="o">=</span><span class="p">[</span><span class="mi">32</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">],</span> <span class="p">)],</span> <span class="n">enabled_precisions</span><span class="o">=</span><span class="p">{</span><span class="n">torch</span><span class="p">.</span><span class="n">half</span><span class="p">})</span>
                
  <span class="n">trt_module</span><span class="p">.</span><span class="n">save</span><span class="p">(</span><span class="s">'test.ts'</span><span class="p">)</span>
</code></pre></div>                </div>
              </li>
              <li>
                <p>summary</p>

                <p>TensorRT는 Nvidia GPU를 활용하여 deploy를 한다면 사용하지 않을 이유가 없을 만큼 최적화를 잘해준다. (network의 precision을 낮추어 조금의 acc 저하는 있지만…) 하지만 변환하면서 문제가 조금씩 있다. 최적화를 지원하지 않는 operation이 있는 경우에는 추가적인 작업이 더 필요하게 된다. 해결할 수 있는 방법으로는 Onnx-Simplifer 라는 tool을 활용하여 해결하는 방법과 직접 cuda 설계하여 최적화하는 방법, torch_tensorrt를 활용하여 변환하는 방법이 있다.</p>

                <ol>
                  <li>Try <a href="https://github.com/daquexian/onnx-simplifier">Onnx-Simplifier</a> 
  <code class="language-plaintext highlighter-rouge">python3 -m onnxsim model.onnx simplified_model.onnx</code></li>
                  <li><a href="https://github.com/NVIDIA/TensorRT">Custom Plugin?</a>, <a href="https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon">Onnx-GraphSurgeon?</a></li>
                  <li>
                    <p>Use Framework integration version (TF-TRT, Torch-TRT)</p>

                    <p>operation 최적화가 안되는 경우에는 operation을 유지시켜서 변환하는 방법</p>
                  </li>
                </ol>
              </li>
            </ul>
          </li>
        </ul>
      </li>
    </ol>
  </li>
</ul>

<h2 id="2-triton">2. Triton</h2>

<blockquote>
  <p>앞서 operation 경량화와 최적화를 통해서 만든 model file을 활용하여 inference API를 만드는 것을 도와주는 툴이다.</p>

</blockquote>

<p>TensorRT 변환과 같이 Triton 또한 환경을 만들기 어려우니 NGC에서 제공해주는 Docker를 적극 활용해보자</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>docker run <span class="nt">--gpus</span><span class="o">=</span><span class="s1">'"device=1"'</span> <span class="nt">--rm</span> <span class="nt">-p8000</span>:8000 <span class="nt">-p8001</span>:8001 <span class="nt">-p8002</span>:8002 <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>/models:/models nvcr.io/nvidia/tritonserver:22.03-py3 tritonserver <span class="nt">--model-repository</span><span class="o">=</span>/models
</code></pre></div></div>

<p>(주의!! 📢 docker를 build하는 directory에 models 폴더가 있어야한다.)</p>

<ul>
  <li>
    <p>models의 directory</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  models
  ├── onnx_model
  │   ├── 1
  │   │   └── model.onnx
  │   └── config.pbtxt
  ├── torch_model
  │   ├── 1
  │   │   └── model.pt
  │   └── config.pbtxt
  └── trt_model
      ├── 1
      │   └── model.plan
      └── config.pbtxt
</code></pre></div>    </div>

    <p>models 폴더 안에는 제공하고자하는 서비스 이름으로 폴더(onnx_model, torch_model, trt_model)가 있고 그 하위의 폴더는 해당 서비스의 버전 이름이다.(1) 그 하위로는 모델 파일이 들어가있다. pbtxt 확장자 파일은 서비스에 대한 config 정보들이 들어있으며 model file의 종류에 따라서 조금 씩 다르다.</p>
  </li>
  <li>
    <p>using port number</p>
    <ul>
      <li>8000 : inference를 위한 http 포트</li>
      <li>8001 : inference를 위한 grpc 포트</li>
      <li>
        <p>8002 : 서비스 운용간에 서비스 현황들을 모니터링 하기 위한  포트(프로메테우스와 연동할 수 있다.)</p>

        <p><img src="/assets/images/triton-handson/Untitled.png" alt="Untitled" /></p>
      </li>
    </ul>
  </li>
</ul>

<p>위 도커를 실행하게 되면 제공하고자 하는 tritonserver가 실행이 되고 해당 컨테이너가 server로서 활용이 가능하게 된다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># optional</span>
<span class="c"># model의 load하는 형식을 주는 것으로</span>
<span class="c"># explicit는 service를 on off를 하는 것을 triton에서 올린 이후 가능</span>
<span class="c"># 권장하는 control mode로 model을 CI/CD를 할때 적절하게 사용할 수 있는 옵션인것으로 생각된다.</span>
<span class="nt">--model-control-mode</span><span class="o">=</span>explicit
<span class="c"># curl로 service on</span>
curl <span class="nt">-X</span> POST localhost:8000/v2/repository/models/onnx_model/load
</code></pre></div></div>

<p>Triton에서는 들어오는 request queue를 처리하는 방법도 제어가 가능하다.</p>

<ol>
  <li>
    <p>Batch scheduler</p>

    <blockquote>
      <p>request의 batch 수에 따라서 가변적으로 변할 수 있게 하는 것으로 특정 size가 만족이 안되면 특정 시간이 지나면 바로 수행하도록 하는 것으로 해당 옵션은max_queue_delay_microseconds로 제어가 가능하다.</p>

    </blockquote>

    <p><img src="/assets/images/triton-handson/2022-07-22_07-12-56.png" alt="스크린샷, 2022-07-22 07-12-56.png" /></p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># 아래 내용을 pdtxt 내용에 포함시키면 실행이 된다.</span>
 dynamic_batching <span class="o">{</span>
   preferred_batch_size: <span class="o">[</span> 4, 8, 16, 32 <span class="o">]</span>
   max_queue_delay_microseconds: 100
 <span class="o">}</span>
</code></pre></div>    </div>
  </li>
  <li>
    <p>Multiple instances</p>

    <blockquote>
      <p>여러 개의 instance를 생성 시켜 병렬로 queue들을 처리하는 방법</p>

    </blockquote>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code> <span class="c"># 아래 내용을 pdtxt 내용에 포함시키면 실행이 된다.</span>
 instance_group <span class="o">[</span>
     <span class="o">{</span>
       count: 1
       kind: KIND_CPU
     <span class="o">}</span>
   <span class="o">]</span>
</code></pre></div>    </div>

    <p><img src="/assets/images/triton-handson/2022-07-22_07-12-42.png" alt="스크린샷, 2022-07-22 07-12-42.png" /></p>
  </li>
</ol>

<ul>
  <li>
    <p>Triton client</p>

    <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  docker run <span class="nt">-it</span> <span class="nt">-v</span> <span class="si">$(</span><span class="nb">pwd</span><span class="si">)</span>:/hands_on <span class="nt">--gpus</span> <span class="s1">'"device=2"'</span> <span class="nt">--net</span><span class="o">=</span>host nvcr.io/nvidia/tritonserver:22.03-py3-sdk
</code></pre></div>    </div>

    <p>triton client는 도커 이름 뒤에 sdk가 붙는 것이 특징이다.</p>

    <ul>
      <li>
        <p>client에서 request 보내기</p>

        <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code>  <span class="kn">import</span> <span class="nn">tritonclient.http</span> <span class="k">as</span> <span class="n">tritonhttpclient</span>
  <span class="kn">import</span> <span class="nn">tritonclient.grpc</span> <span class="k">as</span> <span class="n">tritongrpcclient</span>
  <span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="n">np</span>
  <span class="kn">from</span> <span class="nn">PIL</span> <span class="kn">import</span> <span class="n">Image</span>
  <span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">transforms</span>
  <span class="kn">import</span> <span class="nn">json</span>
        
  <span class="c1">###CONFIGURATION########
</span>  <span class="n">VERBOSE</span> <span class="o">=</span> <span class="bp">False</span>
  <span class="n">input_name</span> <span class="o">=</span> <span class="s">'actual_input_1'</span>
  <span class="n">input_shape</span> <span class="o">=</span> <span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">224</span><span class="p">,</span> <span class="mi">224</span><span class="p">)</span>
  <span class="n">input_dtype</span> <span class="o">=</span> <span class="s">'FP32'</span>
  <span class="n">output_name</span> <span class="o">=</span> <span class="s">'output_1'</span>
  <span class="n">model_name</span> <span class="o">=</span> <span class="s">'trt_model'</span>
  <span class="n">http_url</span> <span class="o">=</span> <span class="s">'localhost:8000'</span>
  <span class="n">grpc_url</span> <span class="o">=</span> <span class="s">'localhost:8001'</span>
  <span class="n">model_version</span> <span class="o">=</span> <span class="s">'1'</span>
  <span class="c1">########################
</span>        
  <span class="c1">#Image Loading
</span>  <span class="n">image</span> <span class="o">=</span> <span class="n">Image</span><span class="p">.</span><span class="nb">open</span><span class="p">(</span><span class="s">'./src/goldfish.jpg'</span><span class="p">)</span>
        
  <span class="n">imagenet_mean</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
  <span class="n">imagenet_std</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.485</span><span class="p">,</span> <span class="mf">0.456</span><span class="p">,</span> <span class="mf">0.406</span><span class="p">]</span>
        
  <span class="n">resize</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Resize</span><span class="p">((</span><span class="mi">256</span><span class="p">,</span> <span class="mi">256</span><span class="p">))</span>
  <span class="n">center_crop</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">CenterCrop</span><span class="p">(</span><span class="mi">224</span><span class="p">)</span>
  <span class="n">to_tensor</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">ToTensor</span><span class="p">()</span>
  <span class="n">normalize</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Normalize</span><span class="p">(</span><span class="n">mean</span><span class="o">=</span><span class="n">imagenet_mean</span><span class="p">,</span>
                                   <span class="n">std</span><span class="o">=</span><span class="n">imagenet_std</span><span class="p">)</span>
        
  <span class="n">transform</span> <span class="o">=</span> <span class="n">transforms</span><span class="p">.</span><span class="n">Compose</span><span class="p">([</span><span class="n">resize</span><span class="p">,</span> <span class="n">center_crop</span><span class="p">,</span> <span class="n">to_tensor</span><span class="p">,</span> <span class="n">normalize</span><span class="p">])</span>
  <span class="n">image_tensor</span> <span class="o">=</span> <span class="n">transform</span><span class="p">(</span><span class="n">image</span><span class="p">).</span><span class="n">unsqueeze</span><span class="p">(</span><span class="mi">0</span><span class="p">).</span><span class="n">cuda</span><span class="p">()</span>
        
  <span class="c1">#Label Loading
</span>        
  <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="s">'./src/imagenet-simple-labels.json'</span><span class="p">)</span> <span class="k">as</span> <span class="nb">file</span><span class="p">:</span>
      <span class="n">labels</span> <span class="o">=</span> <span class="n">json</span><span class="p">.</span><span class="n">load</span><span class="p">(</span><span class="nb">file</span><span class="p">)</span>
        
  <span class="c1">#Start client set up
</span>        
  <span class="c1">#triton_client = tritonhttpclient.InferenceServerClient(url=http_url, verbose=VERBOSE)
</span>  <span class="n">triton_client</span> <span class="o">=</span> <span class="n">tritongrpcclient</span><span class="p">.</span><span class="n">InferenceServerClient</span><span class="p">(</span><span class="n">url</span><span class="o">=</span><span class="n">grpc_url</span><span class="p">,</span> <span class="n">verbose</span><span class="o">=</span><span class="n">VERBOSE</span><span class="p">)</span>
  <span class="n">model_metadata</span> <span class="o">=</span> <span class="n">triton_client</span><span class="p">.</span><span class="n">get_model_metadata</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="n">model_version</span><span class="p">)</span> <span class="c1">#You can remove this line
</span>  <span class="n">model_config</span> <span class="o">=</span> <span class="n">triton_client</span><span class="p">.</span><span class="n">get_model_config</span><span class="p">(</span><span class="n">model_name</span><span class="o">=</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="n">model_version</span><span class="p">)</span>
        
  <span class="n">image_numpy</span> <span class="o">=</span> <span class="n">image_tensor</span><span class="p">.</span><span class="n">cpu</span><span class="p">().</span><span class="n">numpy</span><span class="p">()</span>
  <span class="k">print</span><span class="p">(</span><span class="n">image_numpy</span><span class="p">.</span><span class="n">shape</span><span class="p">)</span>
        
  <span class="c1">#input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)
</span>  <span class="n">input0</span> <span class="o">=</span> <span class="n">tritongrpcclient</span><span class="p">.</span><span class="n">InferInput</span><span class="p">(</span><span class="n">input_name</span><span class="p">,</span> <span class="n">input_shape</span><span class="p">,</span> <span class="n">input_dtype</span><span class="p">)</span>
  <span class="c1">#input0.set_data_from_numpy(image_numpy, binary_data=False)
</span>  <span class="n">input0</span><span class="p">.</span><span class="n">set_data_from_numpy</span><span class="p">(</span><span class="n">image_numpy</span><span class="p">)</span>
        
  <span class="c1">#output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)
</span>  <span class="n">output</span> <span class="o">=</span> <span class="n">tritongrpcclient</span><span class="p">.</span><span class="n">InferRequestedOutput</span><span class="p">(</span><span class="n">output_name</span><span class="p">)</span>
  <span class="n">response</span> <span class="o">=</span> <span class="n">triton_client</span><span class="p">.</span><span class="n">infer</span><span class="p">(</span><span class="n">model_name</span><span class="p">,</span> <span class="n">model_version</span><span class="o">=</span><span class="n">model_version</span><span class="p">,</span> 
                                 <span class="n">inputs</span><span class="o">=</span><span class="p">[</span><span class="n">input0</span><span class="p">],</span> <span class="n">outputs</span><span class="o">=</span><span class="p">[</span><span class="n">output</span><span class="p">])</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">response</span><span class="p">.</span><span class="n">as_numpy</span><span class="p">(</span><span class="n">output_name</span><span class="p">)</span>
  <span class="n">logits</span> <span class="o">=</span> <span class="n">np</span><span class="p">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">logits</span><span class="p">,</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="p">.</span><span class="n">float32</span><span class="p">)</span>
        
  <span class="k">print</span><span class="p">(</span><span class="n">labels</span><span class="p">[</span><span class="n">np</span><span class="p">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">logits</span><span class="p">)])</span>
</code></pre></div>        </div>
      </li>
    </ul>
  </li>
</ul>]]></content><author><name>GitHub User</name></author><category term="triton" /><summary type="html"><![CDATA[Triton은 Nvidia에서 제공하는 Deploy open-source로 request, response로 제공하는 서비스에 대해 다양한 기능들을 제공한다.(model version 관리, service performance test, service batch scheduler and so on) 본 내용은 Nvidia 측에서 만들어주신 자료 기반으로 작성되었습니다. 해당 내용은 https://github.com/leejinho610/TRT_Triton_HandsOn 를 참고]]></summary></entry><entry><title type="html">나만의 Stable Diffusion 서비스 배포</title><link href="http://localhost:4000/bentoml/2022/10/03/Stable-Diffusion-bento.html" rel="alternate" type="text/html" title="나만의 Stable Diffusion 서비스 배포" /><published>2022-10-03T07:42:00-04:00</published><updated>2022-10-03T07:42:00-04:00</updated><id>http://localhost:4000/bentoml/2022/10/03/Stable-Diffusion-bento</id><content type="html" xml:base="http://localhost:4000/bentoml/2022/10/03/Stable-Diffusion-bento.html"><![CDATA[<p><a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a>은 <a href="http://stability.ai/">stability.ai</a>에서 출시한 text-to-image model 오픈소스 입니다. Stable Diffusion은 자연어 프롬프트로 부터 단 몇초만에 창의적인 예술품을 생성할 수 있습니다.</p>

<h2 id="stable-diffusion을-왜-online으로-불러오는-것인가">Stable Diffusion을 왜 Online으로 불러오는 것인가?</h2>

<p>제한적인 local 컴퓨터에서 Stable Diffusion 모델이 좋은 품질의 이미지를 생성하기 위해 오랜 시간이 필요합니다. 모델을 온라인 클라우드 서비스에서 실행하게 된다면 사실상 제한없는 컴퓨터 자원을 사용할 수 있게 되어 높은 품질의 결과물을 보다 빠르게 얻을 수 있습니다. Microservice 단위로 모델을 호스팅하면 <a href="https://modelserving.com/blog/why-do-people-say-its-so-hard-to-deploy-a-ml-model-to-production">ML 모델을 온라인에서 실행해야하는 복잡한 절차</a> 없이 모델 성능을 레버리지 할 수 있고 보다 창의적인 애플리케이션을 제작할 수 있습니다.</p>

<h2 id="stable-diffusion을-ec2에서-배포">Stable Diffusion을 EC2에서 배포</h2>

<p>Stable Diffusion 모델을 온라인으로 호스팅하는 한가지 방법은 BentoML 과 AWS의 EC2를 활용하는 것입니다. BentoML은 기계 학습 서비스를 대규모로 구축, 배포 및 운영할 수 있는 오픈소스 플랫폼입니다. 이 글에서는 BentoML을 사용하여 제품에 즉시 사용할 수 있는 Stable Diffusion 서비스를 생성하고 이를 AWS EC2에 배포합니다. 아래는 이 글의 절차들을 수행하면 얻을 수 있는 결과물 입니다.</p>

<p><img src="/assets/images/stable_diffusion/swagger.png" alt="swagger.png" /></p>

<p>RESTful OpenAPI 서비스 <code class="language-plaintext highlighter-rouge">/txt2img</code> (text to image) 와 <code class="language-plaintext highlighter-rouge">/img2img</code>(image + text to image) 앤드포인트들을 갖고 있는 Swagger 유저 인터페이스</p>

<p><img src="/assets/images/stable_diffusion/txt2img.png" alt="txt2img.png" /></p>

<p><code class="language-plaintext highlighter-rouge">/txt2img</code> 엔드포인트를 활용하여 텍스트 프롬프트로 생성된 예시 이미지</p>

<p><img src="/assets/images/stable_diffusion/img2img.png" alt="img2img.png" /></p>

<p><code class="language-plaintext highlighter-rouge">/img2img</code> 엔드포인트를 활용하여 이미지 및 텍스트 프롬프트로 생성된 예시 이미지</p>

<h2 id="prerequisites"><strong>Prerequisites</strong></h2>

<ul>
  <li>Python 3.9 or above</li>
  <li><a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">AWS CLI</a></li>
  <li><a href="https://docs.bentoml.org/en/latest/installation.html">BentoML</a></li>
  <li><a href="https://github.com/bentoml/bentoctl#installation">bentoctl</a></li>
  <li><a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">Terraform</a></li>
  <li><a href="http://docs.docker.com/install">Docker</a></li>
</ul>

<p>코드와 샘플들은 이 글(<em><a href="https://github.com/bentoml/stable-diffusion-bentoml">https://github.com/bentoml/stable-diffusion-bentoml</a></em>)에서 찾을 수 있습니다.</p>

<h2 id="환경-및-stable-diffusion-model-준비하기">환경 및 Stable Diffusion Model 준비하기</h2>

<p>저장소 복제 및 의존성 설치</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bentoml/stable-diffusion-bentoml.git <span class="o">&amp;&amp;</span> <span class="nb">cd </span>stable-diffusion-bentoml
python3 <span class="nt">-m</span> venv venv <span class="o">&amp;&amp;</span> <span class="nb">.</span> venv/bin/activate
pip <span class="nb">install</span> <span class="nt">-U</span> pip
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p>Stable Diffusion model을 선택하여 다운로드하세요. Single Precision(FP32)는 10GB 이상의 VRAM이 있는 CPU 또는 GPU에 적합합니다. Half Precision(FP16)는 10GB VRAM 미만의 GPU에 적합합니다.</p>

<p><strong>Single Precision (FP32)</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>fp32/
curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4.tgz | <span class="nb">tar </span>zxf - <span class="nt">-C</span> models/
</code></pre></div></div>

<p><strong>Half Precision (FP16)</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>fp16/
curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4_fp16.tgz | <span class="nb">tar </span>zxf - <span class="nt">-C</span> models/
</code></pre></div></div>

<h2 id="stable-diffusion-betno-구축">Stable Diffusion Betno 구축</h2>

<p>모델을 RESTful API로써 serve하기 위해 우리는 BentoML service를 만들것입니다. 다음 예제는 예측을 위한 single precision 모델과 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/fp32/service.py">service.py</a> 모듈을 사용하여 비즈니스 로직으로 서비스를 결합합니다. <code class="language-plaintext highlighter-rouge">@svc.api</code> 데코레이션을 활용하여 함수를 APIs로 노출시킬 수 있습니다. 뿐만아니라 input과 output의 type을 지정해줄 수 있습니다. 예를 들어, <code class="language-plaintext highlighter-rouge">txt2img</code> 앤드포인트는 <code class="language-plaintext highlighter-rouge">JSON</code> 을 입력을 받아 <code class="language-plaintext highlighter-rouge">Image</code> 출력을 반환하는 반면 <code class="language-plaintext highlighter-rouge">img2img</code> 앤드포인트는 <code class="language-plaintext highlighter-rouge">Image</code> 와 <code class="language-plaintext highlighter-rouge">JSON</code> 을 입력으로 받아 출력으로 <code class="language-plaintext highlighter-rouge">Image</code> 를 반환합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">svc</span><span class="p">.</span><span class="n">api</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">JSON</span><span class="p">(),</span> <span class="n">output</span><span class="o">=</span><span class="n">Image</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">txt2img</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stable_diffusion_runner</span><span class="p">.</span><span class="n">txt2img</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="o">@</span><span class="n">svc</span><span class="p">.</span><span class="n">api</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">Multipart</span><span class="p">(</span><span class="n">img</span><span class="o">=</span><span class="n">Image</span><span class="p">(),</span> <span class="n">data</span><span class="o">=</span><span class="n">JSON</span><span class="p">()),</span> <span class="n">output</span><span class="o">=</span><span class="n">Image</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">img2img</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stable_diffusion_runner</span><span class="p">.</span><span class="n">img2img</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p>inference logic의 핵심은 <code class="language-plaintext highlighter-rouge">StableDiffusionRunnable</code> 에 정의되어 있습니다. runnable은 모델에서 <code class="language-plaintext highlighter-rouge">txt2img_pipe</code> 및 <code class="language-plaintext highlighter-rouge">img2img_pipe</code> 메서드를 호출하고 필요한 arguments를 전달하는 역할을 합니다. custom runner는 API에서 모델 inference logic을 실행하기 위해 <code class="language-plaintext highlighter-rouge">StableDiffusionRunnable</code> 에서 인스턴스화 됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stable_diffusion_runner</span> <span class="o">=</span> <span class="n">bentoml</span><span class="p">.</span><span class="n">Runner</span><span class="p">(</span><span class="n">StableDiffusionRunnable</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'stable_diffusion_runner'</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<p>다음 명령어을 실행하여 테스트용 BentoML 서비스를 시작합니다. 로컬의 CPU에서 Stable Diffusion 모델 추론을 실행하는 것은 매우 느립니다. 각 요청을 처리하는데 약 5분이 소요됩니다. 다음 섹션에서는 GPU가 있는 머신에서 서비스를 실행하여 추론 속도를 가속화 하는 방법을 탐구할 것입니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">BENTO_CONFIG</span><span class="o">=</span>configuration.yaml bentoml serve service:svc <span class="nt">--production</span>
</code></pre></div></div>

<p>Curl the text-to-image <code class="language-plaintext highlighter-rouge">/txt2img</code>endpoint.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://127.0.0.1:3000/txt2img <span class="nt">-H</span> <span class="s1">'Content-Type: application/json'</span> <span class="nt">-d</span> <span class="s2">"{</span><span class="se">\"</span><span class="s2">prompt</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">View of a cyberpunk city</span><span class="se">\"</span><span class="s2">}"</span> <span class="nt">--output</span> output.jpg
</code></pre></div></div>

<p>Curl the image-to-image /img2img endpoint.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://127.0.0.1:3000/img2img <span class="nt">-H</span> <span class="s1">'Content-Type: multipart/form-data'</span> <span class="nt">-F</span> <span class="nv">img</span><span class="o">=</span><span class="s2">"@input.jpg"</span> <span class="nt">-F</span> <span class="nv">data</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">prompt</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">View of a cyberpunk city</span><span class="se">\"</span><span class="s2">}"</span> <span class="nt">--output</span> output.jpg
</code></pre></div></div>

<p>필요 파일 및 종속성은 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/fp32/bentofile.yaml">bentoml.yaml</a> 파일에 정의되어 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">service</span><span class="pi">:</span> <span class="s2">"</span><span class="s">service.py:svc"</span>
<span class="na">include</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">service.py"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">requirements.txt"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">models/v1_4"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">configuration.yaml"</span>
<span class="na">python</span><span class="pi">:</span>
  <span class="na">packages</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">torch</span>
    <span class="pi">-</span> <span class="s">transformers</span>
    <span class="pi">-</span> <span class="s">diffusers</span>
    <span class="pi">-</span> <span class="s">ftfy</span>
<span class="na">docker</span><span class="pi">:</span>
  <span class="na">distro</span><span class="pi">:</span> <span class="s">debian</span>
  <span class="na">cuda_version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">11.6.2"</span>
  <span class="na">env</span><span class="pi">:</span>
    <span class="na">BENTOML_CONFIG</span><span class="pi">:</span> <span class="s2">"</span><span class="s">src/configuration.yaml"</span>
</code></pre></div></div>

<p>아래 명령어로 Bento를 만들 수 있습니다. Bento는 BentoML 서비스의 배포 형식입니다. 서비스 실행에 필요한 파일과 설정들을 포함하는 독립적 아카이브 입니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoml build
</code></pre></div></div>

<p>🎉Stable Diffusion bento가 구축되었습니다. 만약 bento를 성공적으로 만들 수 없었다면 걱정하지 마세요 아래 명령어를 이용하여 사전 제작된 bento를 다운로드 할 수 있습니다.</p>

<p><strong>Download Single Precision (FP32) Stable Diffusion Bento</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_fp32.bento <span class="o">&amp;&amp;</span> bentoml import ./sd_fp32.bento
</code></pre></div></div>

<p><strong>Download Half Precision (FP16) Stable Diffusion Bento</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_fp16.bento <span class="o">&amp;&amp;</span> bentoml import ./sd_fp16.bento
</code></pre></div></div>

<h2 id="ec2에서-stable-diffusion-bento-배포">EC2에서 Stable Diffusion Bento 배포</h2>

<p>우리는 <a href="https://github.com/bentoml/bentoctl">bentoctl</a>을 이용하여 bento를 EC2에 배포할 것입니다. <code class="language-plaintext highlighter-rouge">bentoctl</code>는 당신의 bento들을 Terraform로 클라우드 플랫폼에 배포하는 것을 돕습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl operator <span class="nb">install </span>aws-ec2
</code></pre></div></div>

<p>배포를 위한 설정들이 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/bentoctl/deployment_config.yaml">deployment_config.yaml</a> 파일에 구성되어 있습니다. 해당 사양들을 자유롭게 업데이트 해주세요. 기본 설정으로 <code class="language-plaintext highlighter-rouge">us-west-1</code> region에 <em>Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) AMI</em>가 있는 <a href="https://aws.amazon.com/ec2/instance-types/g4/">g4dn.xlarge</a> 인스턴스에 Bento가 배포하도록 구성되어 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">api_version</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">stable-diffusion-demo</span>
<span class="na">operator</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">aws-ec2</span>
<span class="na">template</span><span class="pi">:</span> <span class="s">terraform</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">region</span><span class="pi">:</span> <span class="s">us-west-1</span>
  <span class="na">instance_type</span><span class="pi">:</span> <span class="s">g4dn.2xlarge</span>
  <span class="c1"># points to Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) 20220913 AMI</span>
  <span class="na">ami_id</span><span class="pi">:</span> <span class="s">ami-0a85a3a3fb34b3c7f</span>
  <span class="na">enable_gpus</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p>Terraform 파일 생성</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl generate <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<p>Docker 이미지를 만들고 AWS ECR로 push 합니다. 이미지 업로드는 대역폭에 따라 시간이 오래 걸릴 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl build <span class="nt">-b</span> stable_diffusion_fp32:latest
</code></pre></div></div>

<p>AWS EC2에 bento를 배포하기 위해 Terraform 파일을 등록합니다. EC2 콘솔에서 브라우저를 퍼블릭 IP 주소로 공개하여 Swagger UI에 접근이 가능합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl apply <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<p>마지막으로 Stable Diffusion BentoML 서비스가 더 이상 필요없다면 배포를 삭제합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl destroy <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<h1 id="결론">결론</h1>

<p>이 글에서 저희는 BentoML을 사용하여 Stable Diffusion을 위한 production-ready 서비스를 구축하고 AWS EC2에 배포했습니다. AWS EC2에 서비스를 배포함으로써 더 강력한 하드웨어에서 Stable Diffusion 모델을 짧은 지연시간으로 이미지를 생성하고 단일 시스템 이상으로 확장할 수 있었습니다. 이 글을 재미있게 읽었다면 <a href="https://github.com/bentoml/BentoML">github의 Bentoml project</a>에 ⭐ 와 <a href="https://l.bentoml.com/join-slack">slack community</a>에서 마음에 맞는 분들을 만나보시길 바랍니다.</p>

<h1 id="reference">Reference</h1>
<p>해당 자료는 BentoML Blog를 번역한 자료 입니다.
<a href="https://modelserving.com/blog/deploying-your-own-stable-diffusion-service-mz9wk">원본 자료</a></p>]]></content><author><name>GitHub User</name></author><category term="BentoML" /><summary type="html"><![CDATA[Stable Diffusion은 stability.ai에서 출시한 text-to-image model 오픈소스 입니다. Stable Diffusion은 자연어 프롬프트로 부터 단 몇초만에 창의적인 예술품을 생성할 수 있습니다.]]></summary></entry></feed>