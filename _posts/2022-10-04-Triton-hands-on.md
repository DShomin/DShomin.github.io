---
layout: post
title:  "Triton hands-on"
date:   2022-10-04 18:40:00 +0900
categories: triton
tags: Triton ModelServing NVIDIA TensorRT Inference
---

> Tritonì€ Nvidiaì—ì„œ ì œê³µí•˜ëŠ” Deploy open-sourceë¡œ request, responseë¡œ ì œê³µí•˜ëŠ” ì„œë¹„ìŠ¤ì— ëŒ€í•´ ë‹¤ì–‘í•œ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•œë‹¤.(model version ê´€ë¦¬, service performance test, service batch scheduler and so on) ë³¸ ë‚´ìš©ì€ Nvidia ì¸¡ì—ì„œ ë§Œë“¤ì–´ì£¼ì‹  ìë£Œ ê¸°ë°˜ìœ¼ë¡œ ì‘ì„±ë˜ì—ˆìŠµë‹ˆë‹¤. í•´ë‹¹ ë‚´ìš©ì€ https://github.com/leejinho610/TRT_Triton_HandsOn ë¥¼ ì°¸ê³ 
> 

## 1. PyTorch model convert

> modelì„ deploy ì‹œ model file frameworkê°€ ë‹¤ì–‘í•˜ë‹¤. ì´ë²ˆ hands-onì—ì„œëŠ” PyTorch ê´€ë ¨í•œ frameworkë¥¼ ë‹¤ë£° ì˜ˆì •ì´ë‹¤.(torch-script, Onnx, tensorRT)
> 
- base model
    
    hands-onê°„ì— ì‚¬ìš©í•˜ê³ ì í•˜ëŠ” base model ì´ë‹¤.
    
    ```python
    from torchvision import models
    
    model = models.wide_resnet101_2(pretrained=True).eval().cuda()
    ```
    
- Torch-script
    
    torch-scriptëŠ” pytorch modelì„ ë³´ë‹¤ ê°€ì†í™”í•˜ê¸° ìœ„í•œ frameworkë¡œ C++ì—ì„œë„ ì‚¬ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. ë³´ë‹¤ ìì„¸í•œ ë‚´ìš©ì€ [pytorch ê³µì‹ documentation](https://pytorch.org/docs/stable/jit.html)ì„ ì°¸ê³ í•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒ ê°™ë‹¤. 
    
    ```python
    script_model = torch.jit.script(model)
    script_model.save('model.pt')
    ```
    
- Onnx
    
    ```python
    input_names = ["actual_input_1"]
    output_names = ["output_1"]
    torch.onnx.export(model, torch.randn(1, 3, 224, 224).cuda(), 'model.onnx',
                      input_names=input_names, output_names=output_names,
                      dynamic_axes={'actual_input_1':{0:'batch_size'}, 'output_1': {0:'batch_size'}})
    ```
    
- tensorRT
    
    tensorRTëŠ” Nvidia GPUì— ìµœì í™”í•˜ëŠ” íˆ´ë¡œ ë°©ë²•ì´ í¬ê²Œ ë‘ ê°€ì§€ê°€ ìˆë‹¤. (1. trtexec í™œìš© 2. torchtrt í™œìš©)
    
    ğŸ“¢ ì£¼ì˜!! tensorRTëŠ” targetì´ ë˜ëŠ” GPUì—ì„œ ì§„í–‰í•´ì•¼ í•œë‹¤. 
    
    ë‚˜ìœ ì˜ˆ) tensorRT ë³€í™˜ì€ 2080TIì—ì„œ ì§„í–‰í•˜ê³  ë³€í™˜í•œ íŒŒì¼ì„ 3080TIì—ì„œ deploy 
    
    1. trtexec
        
        > í•„ìê°€ ì§ì ‘ í™˜ê²½ì„ ì„¤ì¹˜í•˜ì—¬ tensorRT ë³€í™˜í•˜ëŠ” í™˜ê²½ì„ ë§Œë“¤ì–´ ë´¤ì§€ë§Œ ë§ì´ ì–´ë ¤ì› ìœ¼ë©° ê²°êµ­ ì‹¤íŒ¨í•˜ì˜€ë‹¤. ì˜¤ê¸°ë¡œ ì‹œì‘í•˜ì˜€ì§€ë§Œ ë¬´ìˆ˜í•œ ì• ëŸ¬ë¥¼ ë³´ê³  ì»´í“¨í„°ì—ê²Œ ì¡Œë‹¤â€¦ ë³€í™˜í•˜ëŠ” ê²ƒì€ dockerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ì¶”ì²œí•˜ë©° ì•„ë˜ docker commandë¥¼ ì‹¤í–‰í•˜ë©´ ì‰½ê²Œ ë³€í™˜ì´ ê°€ëŠ¥í•˜ë‹¤.
        > 
        
        ```bash
        docker run --gpus '"device=0"' -it --rm -p 8887:8887 -v $(pwd):/hands_on [nvcr.io/nvidia/pytorch:22.03-py3](http://nvcr.io/nvidia/pytorch:22.03-py3)
        ```
        
        - TensorRT ë³€í™˜
            - using trtexec
                
                ```bash
                trtexec \
                  --onnx=model.onnx \
                  --explicitBatch \
                  --optShapes=actual_input_1:16x3x224x224 \
                  --maxShapes=actual_input_1:32x3x224x224 \
                  --minShapes=actual_input_1:1x3x224x224 \
                  --best \
                  --saveEngine=model.plan \
                  --workspace=4096
                ```
                
                - argument description
                    - onnx : model fileì˜ path
                    - explicitBatch
                    - optShapes : ìµœì í™”í•˜ê³ ì í•˜ëŠ” batch shape
                    - maxShapes : ìµœëŒ€ batch shape
                    - minShapes : ìµœì†Œ batch shape
                    - best : networkì˜ precisionì„ ê²°ì •í•˜ëŠ” argumentë¡œ defaultê°€ bestë¥¼ í™œì„±í™”í•œ ê²ƒì´ë©° --fp16, --int8, --noTF32 ì™€ ê°™ë‹¤.
                    - saveEngine : tensorRT ë³€í™˜ í›„ ì €ì¥í•˜ê³ ì í•˜ëŠ” ì´ë¦„
                    - workspace : ìµœì í™”ë¥¼ ìœ„í•˜ì—¬ í• ë‹¹í•˜ê³ ì í•˜ëŠ” memory sizeì´ë©° ì´í›„ ë²„ì „ ë¶€í„° ì‚­ì œì˜ˆì •
                
                ---
                
                - trtexec dummy test
                    
                    > ë³€í™˜ ì´í›„ ë³€í™˜í•œ tensorRT íŒŒì¼ì´ ì–´ëŠì •ë„ ì„±ëŠ¥ì´ ë‚˜ì˜¤ëŠ”ì§€ í•œì¤„ ì½”ë“œë¡œ ìˆ˜í–‰ì´ ê°€ëŠ¥í•˜ë‹¤.
                    > 
                    
                    ```bash
                    # 1. dummy inputìœ¼ë¡œ inference ì‹œê°„ ì¸¡ì •
                    trtexec --loadEngine=model.plan --dumpOutput
                    
                    # 2. dummy inputìœ¼ë¡œ layer ë³„ ì‹œê°„ ì¸¡ì •
                    trtexec --loadEngine=model.plan --dumpProfile
                    ```
                    
            
            - torchtrt libray
                
                pythonì˜ libraryë¡œ ì œê³µë˜ëŠ” ë°©ë²•ìœ¼ë¡œ ì‰½ê²Œ ì ìš©í•  ìˆ˜ ìˆì§€ë§Œ ë³€í™˜ í›„ performanceëŠ” trtexec ë³´ë‹¤ ë–¨ì–´ì§€ëŠ” ê²ƒìœ¼ë¡œ ì•Œë ¤ì ¸ìˆë‹¤.
                
                ```python
                #Optional
                import torch_tensorrt as torchtrt
                
                model = models.wide_resnet101_2(pretrained=True).eval().cuda()
                #Or you can load torchscript file directly likes
                #model = torch.jit.load('ts_model_path')
                
                trt_module = torchtrt.compile(model, inputs=[torchtrt.Input(
                                                min_shape=[1, 3, 224, 224],
                                                opt_shape=[16, 3, 224, 224],
                                                max_shape=[32, 3, 224, 224], )], enabled_precisions={torch.half})
                
                trt_module.save('test.ts')
                ```
                
            - summary
                
                TensorRTëŠ” Nvidia GPUë¥¼ í™œìš©í•˜ì—¬ deployë¥¼ í•œë‹¤ë©´ ì‚¬ìš©í•˜ì§€ ì•Šì„ ì´ìœ ê°€ ì—†ì„ ë§Œí¼ ìµœì í™”ë¥¼ ì˜í•´ì¤€ë‹¤. (networkì˜ precisionì„ ë‚®ì¶”ì–´ ì¡°ê¸ˆì˜ acc ì €í•˜ëŠ” ìˆì§€ë§Œâ€¦) í•˜ì§€ë§Œ ë³€í™˜í•˜ë©´ì„œ ë¬¸ì œê°€ ì¡°ê¸ˆì”© ìˆë‹¤. ìµœì í™”ë¥¼ ì§€ì›í•˜ì§€ ì•ŠëŠ” operationì´ ìˆëŠ” ê²½ìš°ì—ëŠ” ì¶”ê°€ì ì¸ ì‘ì—…ì´ ë” í•„ìš”í•˜ê²Œ ëœë‹¤. í•´ê²°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ìœ¼ë¡œëŠ” Onnx-Simplifer ë¼ëŠ” toolì„ í™œìš©í•˜ì—¬ í•´ê²°í•˜ëŠ” ë°©ë²•ê³¼ ì§ì ‘ cuda ì„¤ê³„í•˜ì—¬ ìµœì í™”í•˜ëŠ” ë°©ë²•, torch_tensorrtë¥¼ í™œìš©í•˜ì—¬ ë³€í™˜í•˜ëŠ” ë°©ë²•ì´ ìˆë‹¤.
                
                1. Try [Onnx-Simplifier](https://github.com/daquexian/onnx-simplifier)
                `python3 -m onnxsim model.onnx simplified_model.onnx`
                2. [Custom Plugin?](https://github.com/NVIDIA/TensorRT), [Onnx-GraphSurgeon?](https://github.com/NVIDIA/TensorRT/tree/master/tools/onnx-graphsurgeon)
                3. Use Framework integration version (TF-TRT, Torch-TRT)
                    
                    operation ìµœì í™”ê°€ ì•ˆë˜ëŠ” ê²½ìš°ì—ëŠ” operationì„ ìœ ì§€ì‹œì¼œì„œ ë³€í™˜í•˜ëŠ” ë°©ë²•
                    

## 2. Triton

> ì•ì„œ operation ê²½ëŸ‰í™”ì™€ ìµœì í™”ë¥¼ í†µí•´ì„œ ë§Œë“  model fileì„ í™œìš©í•˜ì—¬ inference APIë¥¼ ë§Œë“œëŠ” ê²ƒì„ ë„ì™€ì£¼ëŠ” íˆ´ì´ë‹¤.
> 

TensorRT ë³€í™˜ê³¼ ê°™ì´ Triton ë˜í•œ í™˜ê²½ì„ ë§Œë“¤ê¸° ì–´ë ¤ìš°ë‹ˆ NGCì—ì„œ ì œê³µí•´ì£¼ëŠ” Dockerë¥¼ ì ê·¹ í™œìš©í•´ë³´ì

```bash
docker run --gpus='"device=1"' --rm -p8000:8000 -p8001:8001 -p8002:8002 -v $(pwd)/models:/models nvcr.io/nvidia/tritonserver:22.03-py3 tritonserver --model-repository=/models
```

(ì£¼ì˜!! ğŸ“¢ dockerë¥¼ buildí•˜ëŠ” directoryì— models í´ë”ê°€ ìˆì–´ì•¼í•œë‹¤.)

- modelsì˜ directory
    
    ```bash
    models
    â”œâ”€â”€ onnx_model
    â”‚Â Â  â”œâ”€â”€ 1
    â”‚Â Â  â”‚Â Â  â””â”€â”€ model.onnx
    â”‚Â Â  â””â”€â”€ config.pbtxt
    â”œâ”€â”€ torch_model
    â”‚Â Â  â”œâ”€â”€ 1
    â”‚Â Â  â”‚Â Â  â””â”€â”€ model.pt
    â”‚Â Â  â””â”€â”€ config.pbtxt
    â””â”€â”€ trt_model
        â”œâ”€â”€ 1
        â”‚Â Â  â””â”€â”€ model.plan
        â””â”€â”€ config.pbtxt
    ```
    
    models í´ë” ì•ˆì—ëŠ” ì œê³µí•˜ê³ ìí•˜ëŠ” ì„œë¹„ìŠ¤ ì´ë¦„ìœ¼ë¡œ í´ë”(onnx_model, torch_model, trt_model)ê°€ ìˆê³  ê·¸ í•˜ìœ„ì˜ í´ë”ëŠ” í•´ë‹¹ ì„œë¹„ìŠ¤ì˜ ë²„ì „ ì´ë¦„ì´ë‹¤.(1) ê·¸ í•˜ìœ„ë¡œëŠ” ëª¨ë¸ íŒŒì¼ì´ ë“¤ì–´ê°€ìˆë‹¤. pbtxt í™•ì¥ì íŒŒì¼ì€ ì„œë¹„ìŠ¤ì— ëŒ€í•œ config ì •ë³´ë“¤ì´ ë“¤ì–´ìˆìœ¼ë©° model fileì˜ ì¢…ë¥˜ì— ë”°ë¼ì„œ ì¡°ê¸ˆ ì”© ë‹¤ë¥´ë‹¤.
    
- using port number
    - 8000 : inferenceë¥¼ ìœ„í•œ http í¬íŠ¸
    - 8001 : inferenceë¥¼ ìœ„í•œ grpc í¬íŠ¸
    - 8002 : ì„œë¹„ìŠ¤ ìš´ìš©ê°„ì— ì„œë¹„ìŠ¤ í˜„í™©ë“¤ì„ ëª¨ë‹ˆí„°ë§ í•˜ê¸° ìœ„í•œ  í¬íŠ¸(í”„ë¡œë©”í…Œìš°ìŠ¤ì™€ ì—°ë™í•  ìˆ˜ ìˆë‹¤.)
        
        ![Untitled](/assets/images/triton-handson/Untitled.png)
        

ìœ„ ë„ì»¤ë¥¼ ì‹¤í–‰í•˜ê²Œ ë˜ë©´ ì œê³µí•˜ê³ ì í•˜ëŠ” tritonserverê°€ ì‹¤í–‰ì´ ë˜ê³  í•´ë‹¹ ì»¨í…Œì´ë„ˆê°€ serverë¡œì„œ í™œìš©ì´ ê°€ëŠ¥í•˜ê²Œ ëœë‹¤. 

```bash
# optional
# modelì˜ loadí•˜ëŠ” í˜•ì‹ì„ ì£¼ëŠ” ê²ƒìœ¼ë¡œ
# explicitëŠ” serviceë¥¼ on offë¥¼ í•˜ëŠ” ê²ƒì„ tritonì—ì„œ ì˜¬ë¦° ì´í›„ ê°€ëŠ¥
# ê¶Œì¥í•˜ëŠ” control modeë¡œ modelì„ CI/CDë¥¼ í• ë•Œ ì ì ˆí•˜ê²Œ ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” ì˜µì…˜ì¸ê²ƒìœ¼ë¡œ ìƒê°ëœë‹¤.
--model-control-mode=explicit
# curlë¡œ service on
curl -X POST localhost:8000/v2/repository/models/onnx_model/load
```

Tritonì—ì„œëŠ” ë“¤ì–´ì˜¤ëŠ” request queueë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•ë„ ì œì–´ê°€ ê°€ëŠ¥í•˜ë‹¤. 

1. Batch scheduler
    
    > requestì˜ batch ìˆ˜ì— ë”°ë¼ì„œ ê°€ë³€ì ìœ¼ë¡œ ë³€í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒìœ¼ë¡œ íŠ¹ì • sizeê°€ ë§Œì¡±ì´ ì•ˆë˜ë©´ íŠ¹ì • ì‹œê°„ì´ ì§€ë‚˜ë©´ ë°”ë¡œ ìˆ˜í–‰í•˜ë„ë¡ í•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ë‹¹ ì˜µì…˜ì€max_queue_delay_microsecondsë¡œ ì œì–´ê°€ ê°€ëŠ¥í•˜ë‹¤.
    > 
    
    ![ìŠ¤í¬ë¦°ìƒ·, 2022-07-22 07-12-56.png](/assets/images/triton-handson/2022-07-22_07-12-56.png)
    
    ```bash
    # ì•„ë˜ ë‚´ìš©ì„ pdtxt ë‚´ìš©ì— í¬í•¨ì‹œí‚¤ë©´ ì‹¤í–‰ì´ ëœë‹¤.
    dynamic_batching {
      preferred_batch_size: [ 4, 8, 16, 32 ]
      max_queue_delay_microseconds: 100
    }
    ```
    
2. Multiple instances
    
    > ì—¬ëŸ¬ ê°œì˜ instanceë¥¼ ìƒì„± ì‹œì¼œ ë³‘ë ¬ë¡œ queueë“¤ì„ ì²˜ë¦¬í•˜ëŠ” ë°©ë²•
    > 
    
    ```bash
    # ì•„ë˜ ë‚´ìš©ì„ pdtxt ë‚´ìš©ì— í¬í•¨ì‹œí‚¤ë©´ ì‹¤í–‰ì´ ëœë‹¤.
    instance_group [
        {
          count: 1
          kind: KIND_CPU
        }
      ]
    ```
    
    ![ìŠ¤í¬ë¦°ìƒ·, 2022-07-22 07-12-42.png](/assets/images/triton-handson/2022-07-22_07-12-42.png)
    

- Triton client
    
    ```bash
    docker run -it -v $(pwd):/hands_on --gpus '"device=2"' --net=host nvcr.io/nvidia/tritonserver:22.03-py3-sdk
    ```
    
    triton clientëŠ” ë„ì»¤ ì´ë¦„ ë’¤ì— sdkê°€ ë¶™ëŠ” ê²ƒì´ íŠ¹ì§•ì´ë‹¤.
    
    - clientì—ì„œ request ë³´ë‚´ê¸°
        
        ```python
        import tritonclient.http as tritonhttpclient
        import tritonclient.grpc as tritongrpcclient
        import numpy as np
        from PIL import Image
        from torchvision import transforms
        import json
        
        ###CONFIGURATION########
        VERBOSE = False
        input_name = 'actual_input_1'
        input_shape = (1, 3, 224, 224)
        input_dtype = 'FP32'
        output_name = 'output_1'
        model_name = 'trt_model'
        http_url = 'localhost:8000'
        grpc_url = 'localhost:8001'
        model_version = '1'
        ########################
        
        #Image Loading
        image = Image.open('./src/goldfish.jpg')
        
        imagenet_mean = [0.485, 0.456, 0.406]
        imagenet_std = [0.485, 0.456, 0.406]
        
        resize = transforms.Resize((256, 256))
        center_crop = transforms.CenterCrop(224)
        to_tensor = transforms.ToTensor()
        normalize = transforms.Normalize(mean=imagenet_mean,
                                         std=imagenet_std)
        
        transform = transforms.Compose([resize, center_crop, to_tensor, normalize])
        image_tensor = transform(image).unsqueeze(0).cuda()
        
        #Label Loading
        
        with open('./src/imagenet-simple-labels.json') as file:
            labels = json.load(file)
        
        #Start client set up
        
        #triton_client = tritonhttpclient.InferenceServerClient(url=http_url, verbose=VERBOSE)
        triton_client = tritongrpcclient.InferenceServerClient(url=grpc_url, verbose=VERBOSE)
        model_metadata = triton_client.get_model_metadata(model_name=model_name, model_version=model_version) #You can remove this line
        model_config = triton_client.get_model_config(model_name=model_name, model_version=model_version)
        
        image_numpy = image_tensor.cpu().numpy()
        print(image_numpy.shape)
        
        #input0 = tritonhttpclient.InferInput(input_name, input_shape, input_dtype)
        input0 = tritongrpcclient.InferInput(input_name, input_shape, input_dtype)
        #input0.set_data_from_numpy(image_numpy, binary_data=False)
        input0.set_data_from_numpy(image_numpy)
        
        #output = tritonhttpclient.InferRequestedOutput(output_name, binary_data=False)
        output = tritongrpcclient.InferRequestedOutput(output_name)
        response = triton_client.infer(model_name, model_version=model_version, 
                                       inputs=[input0], outputs=[output])
        logits = response.as_numpy(output_name)
        logits = np.asarray(logits, dtype=np.float32)
        
        print(labels[np.argmax(logits)])
        ```