I"nJ<p><a href="https://stability.ai/blog/stable-diffusion-public-release">Stable Diffusion</a>은 <a href="http://stability.ai/">stability.ai</a>에서 출시한 text-to-image model 오픈소스 입니다. Stable Diffusion은 자연어 프롬프트로 부터 단 몇초만에 창의적인 예술품을 생성할 수 있습니다.</p>

<h2 id="stable-diffusion을-왜-online으로-불러오는-것인가">Stable Diffusion을 왜 Online으로 불러오는 것인가?</h2>

<p>제한적인 local 컴퓨터에서 Stable Diffusion 모델이 좋은 품질의 이미지를 생성하기 위해 오랜 시간이 필요합니다. 모델을 온라인 클라우드 서비스에서 실행하게 된다면 사실상 제한없는 컴퓨터 자원을 사용할 수 있게 되어 높은 품질의 결과물을 보다 빠르게 얻을 수 있습니다. Microservice 단위로 모델을 호스팅하면 <a href="https://modelserving.com/blog/why-do-people-say-its-so-hard-to-deploy-a-ml-model-to-production">ML 모델을 온라인에서 실행해야하는 복잡한 절차</a> 없이 모델 성능을 레버리지 할 수 있고 보다 창의적인 애플리케이션을 제작할 수 있습니다.</p>

<h2 id="stable-diffusion을-ec2에서-배포">Stable Diffusion을 EC2에서 배포</h2>

<p>Stable Diffusion 모델을 온라인으로 호스팅하는 한가지 방법은 BentoML 과 AWS의 EC2를 활용하는 것입니다. BentoML은 기계 학습 서비스를 대규모로 구축, 배포 및 운영할 수 있는 오픈소스 플랫폼입니다. 이 글에서는 BentoML을 사용하여 제품에 즉시 사용할 수 있는 Stable Diffusion 서비스를 생성하고 이를 AWS EC2에 배포합니다. 아래는 이 글의 절차들을 수행하면 얻을 수 있는 결과물 입니다.</p>

<p><img src="/assets/images/stable_diffusion/swagger.png" alt="swagger.png" /></p>

<p>RESTful OpenAPI 서비스 <code class="language-plaintext highlighter-rouge">/txt2img</code> (text to image) 와 <code class="language-plaintext highlighter-rouge">/img2img</code>(image + text to image) 앤드포인트들을 갖고 있는 Swagger 유저 인터페이스</p>

<p><img src="/assets/images/stable_diffusion/txt2img.png" alt="txt2img.png" /></p>

<p><code class="language-plaintext highlighter-rouge">/txt2img</code> 엔드포인트를 활용하여 텍스트 프롬프트로 생성된 예시 이미지</p>

<p><img src="/assets/images/stable_diffusion/img2img.png" alt="img2img.png" /></p>

<p><code class="language-plaintext highlighter-rouge">/img2img</code> 엔드포인트를 활용하여 이미지 및 텍스트 프롬프트로 생성된 예시 이미지</p>

<h2 id="prerequisites"><strong>Prerequisites</strong></h2>

<ul>
  <li>Python 3.9 or above</li>
  <li><a href="https://docs.aws.amazon.com/cli/latest/userguide/getting-started-install.html">AWS CLI</a></li>
  <li><a href="https://docs.bentoml.org/en/latest/installation.html">BentoML</a></li>
  <li><a href="https://github.com/bentoml/bentoctl#installation">bentoctl</a></li>
  <li><a href="https://learn.hashicorp.com/tutorials/terraform/install-cli">Terraform</a></li>
  <li><a href="http://docs.docker.com/install">Docker</a></li>
</ul>

<p>코드와 샘플들은 이 글(<em><a href="https://github.com/bentoml/stable-diffusion-bentoml">https://github.com/bentoml/stable-diffusion-bentoml</a></em>)에서 찾을 수 있습니다.</p>

<h2 id="환경-및-stable-diffusion-model-준비하기">환경 및 Stable Diffusion Model 준비하기</h2>

<p>저장소 복제 및 의존성 설치</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>git clone https://github.com/bentoml/stable-diffusion-bentoml.git <span class="o">&amp;&amp;</span> <span class="nb">cd </span>stable-diffusion-bentoml
python3 <span class="nt">-m</span> venv venv <span class="o">&amp;&amp;</span> <span class="nb">.</span> venv/bin/activate
pip <span class="nb">install</span> <span class="nt">-U</span> pip
pip <span class="nb">install</span> <span class="nt">-r</span> requirements.txt
</code></pre></div></div>

<p>Stable Diffusion model을 선택하여 다운로드하세요. Single Precision(FP32)는 10GB 이상의 VRAM이 있는 CPU 또는 GPU에 적합합니다. Half Precision(FP16)는 10GB VRAM 미만의 GPU에 적합합니다.</p>

<p><strong>Single Precision (FP32)</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>fp32/
curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4.tgz | <span class="nb">tar </span>zxf - <span class="nt">-C</span> models/
</code></pre></div></div>

<p><strong>Half Precision (FP16)</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">cd </span>fp16/
curl https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_model_v1_4_fp16.tgz | <span class="nb">tar </span>zxf - <span class="nt">-C</span> models/
</code></pre></div></div>

<h2 id="stable-diffusion-betno-구축">Stable Diffusion Betno 구축</h2>

<p>모델을 RESTful API로써 serve하기 위해 우리는 BentoML service를 만들것입니다. 다음 예제는 예측을 위한 single precision 모델과 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/fp32/service.py">service.py</a> 모듈을 사용하여 비즈니스 로직으로 서비스를 결합합니다. <code class="language-plaintext highlighter-rouge">@svc.api</code> 데코레이션을 활용하여 함수를 APIs로 노출시킬 수 있습니다. 뿐만아니라 input과 output의 type을 지정해줄 수 있습니다. 예를 들어, <code class="language-plaintext highlighter-rouge">txt2img</code> 앤드포인트는 <code class="language-plaintext highlighter-rouge">JSON</code> 을 입력을 받아 <code class="language-plaintext highlighter-rouge">Image</code> 출력을 반환하는 반면 <code class="language-plaintext highlighter-rouge">img2img</code> 앤드포인트는 <code class="language-plaintext highlighter-rouge">Image</code> 와 <code class="language-plaintext highlighter-rouge">JSON</code> 을 입력으로 받아 출력으로 <code class="language-plaintext highlighter-rouge">Image</code> 를 반환합니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="o">@</span><span class="n">svc</span><span class="p">.</span><span class="n">api</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">JSON</span><span class="p">(),</span> <span class="n">output</span><span class="o">=</span><span class="n">Image</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">txt2img</span><span class="p">(</span><span class="n">input_data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stable_diffusion_runner</span><span class="p">.</span><span class="n">txt2img</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">input_data</span><span class="p">)</span>

<span class="o">@</span><span class="n">svc</span><span class="p">.</span><span class="n">api</span><span class="p">(</span><span class="nb">input</span><span class="o">=</span><span class="n">Multipart</span><span class="p">(</span><span class="n">img</span><span class="o">=</span><span class="n">Image</span><span class="p">(),</span> <span class="n">data</span><span class="o">=</span><span class="n">JSON</span><span class="p">()),</span> <span class="n">output</span><span class="o">=</span><span class="n">Image</span><span class="p">())</span>
<span class="k">def</span> <span class="nf">img2img</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">data</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">stable_diffusion_runner</span><span class="p">.</span><span class="n">img2img</span><span class="p">.</span><span class="n">run</span><span class="p">(</span><span class="n">img</span><span class="p">,</span> <span class="n">data</span><span class="p">)</span>
</code></pre></div></div>

<p>inference logic의 핵심은 <code class="language-plaintext highlighter-rouge">StableDiffusionRunnable</code> 에 정의되어 있습니다. runnable은 모델에서 <code class="language-plaintext highlighter-rouge">txt2img_pipe</code> 및 <code class="language-plaintext highlighter-rouge">img2img_pipe</code> 메서드를 호출하고 필요한 arguments를 전달하는 역할을 합니다. custom runner는 API에서 모델 inference logic을 실행하기 위해 <code class="language-plaintext highlighter-rouge">StableDiffusionRunnable</code> 에서 인스턴스화 됩니다.</p>

<div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">stable_diffusion_runner</span> <span class="o">=</span> <span class="n">bentoml</span><span class="p">.</span><span class="n">Runner</span><span class="p">(</span><span class="n">StableDiffusionRunnable</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'stable_diffusion_runner'</span><span class="p">,</span> <span class="n">max_batch_size</span><span class="o">=</span><span class="mi">10</span><span class="p">)</span>
</code></pre></div></div>

<p>다음 명령어을 실행하여 테스트용 BentoML 서비스를 시작합니다. 로컬의 CPU에서 Stable Diffusion 모델 추론을 실행하는 것은 매우 느립니다. 각 요청을 처리하는데 약 5분이 소요됩니다. 다음 섹션에서는 GPU가 있는 머신에서 서비스를 실행하여 추론 속도를 가속화 하는 방법을 탐구할 것입니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nv">BENTO_CONFIG</span><span class="o">=</span>configuration.yaml bentoml serve service:svc <span class="nt">--production</span>
</code></pre></div></div>

<p>Curl the text-to-image <code class="language-plaintext highlighter-rouge">/txt2img</code>endpoint.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://127.0.0.1:3000/txt2img <span class="nt">-H</span> <span class="s1">'Content-Type: application/json'</span> <span class="nt">-d</span> <span class="s2">"{</span><span class="se">\"</span><span class="s2">prompt</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">View of a cyberpunk city</span><span class="se">\"</span><span class="s2">}"</span> <span class="nt">--output</span> output.jpg
</code></pre></div></div>

<p>Curl the image-to-image /img2img endpoint.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-X</span> POST http://127.0.0.1:3000/img2img <span class="nt">-H</span> <span class="s1">'Content-Type: multipart/form-data'</span> <span class="nt">-F</span> <span class="nv">img</span><span class="o">=</span><span class="s2">"@input.jpg"</span> <span class="nt">-F</span> <span class="nv">data</span><span class="o">=</span><span class="s2">"{</span><span class="se">\"</span><span class="s2">prompt</span><span class="se">\"</span><span class="s2">:</span><span class="se">\"</span><span class="s2">View of a cyberpunk city</span><span class="se">\"</span><span class="s2">}"</span> <span class="nt">--output</span> output.jpg
</code></pre></div></div>

<p>필요 파일 및 종속성은 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/fp32/bentofile.yaml">bentoml.yaml</a> 파일에 정의되어 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">service</span><span class="pi">:</span> <span class="s2">"</span><span class="s">service.py:svc"</span>
<span class="na">include</span><span class="pi">:</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">service.py"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">requirements.txt"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">models/v1_4"</span>
  <span class="pi">-</span> <span class="s2">"</span><span class="s">configuration.yaml"</span>
<span class="na">python</span><span class="pi">:</span>
  <span class="na">packages</span><span class="pi">:</span>
    <span class="pi">-</span> <span class="s">torch</span>
    <span class="pi">-</span> <span class="s">transformers</span>
    <span class="pi">-</span> <span class="s">diffusers</span>
    <span class="pi">-</span> <span class="s">ftfy</span>
<span class="na">docker</span><span class="pi">:</span>
  <span class="na">distro</span><span class="pi">:</span> <span class="s">debian</span>
  <span class="na">cuda_version</span><span class="pi">:</span> <span class="s2">"</span><span class="s">11.6.2"</span>
  <span class="na">env</span><span class="pi">:</span>
    <span class="na">BENTOML_CONFIG</span><span class="pi">:</span> <span class="s2">"</span><span class="s">src/configuration.yaml"</span>
</code></pre></div></div>

<p>아래 명령어로 Bento를 만들 수 있습니다. Bento는 BentoML 서비스의 배포 형식입니다. 서비스 실행에 필요한 파일과 설정들을 포함하는 독립적 아카이브 입니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoml build
</code></pre></div></div>

<p>🎉Stable Diffusion bento가 구축되었습니다. 어떠 이유로 bento를 성공적으로 만들 수 없었다면 걱정하지 마세요 아래 명령어를 이용하여 사전 제작된 bento를 다운로드 할 수 있습니다.</p>

<p><strong>Download Single Precision (FP32) Stable Diffusion Bento</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_fp32.bento <span class="o">&amp;&amp;</span> bentoml import ./sd_fp32.bento
</code></pre></div></div>

<p><strong>Download Half Precision (FP16) Stable Diffusion Bento</strong></p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>curl <span class="nt">-O</span> https://s3.us-west-2.amazonaws.com/bentoml.com/stable_diffusion_bentoml/sd_fp16.bento <span class="o">&amp;&amp;</span> bentoml import ./sd_fp16.bento
</code></pre></div></div>

<h2 id="ec2에서-stable-diffusion-bento-배포">EC2에서 Stable Diffusion Bento 배포</h2>

<p>우리는 <a href="https://github.com/bentoml/bentoctl">bentoctl</a>을 이용하여 bento를 EC2에 배포할 것입니다. <code class="language-plaintext highlighter-rouge">bentoctl</code>는 당신의 bento들을 Terraform로 클라우드 플랫폼에 배포하는 것을 돕습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl operator <span class="nb">install </span>aws-ec2
</code></pre></div></div>

<p>배포를 위한 설정들이 <a href="https://github.com/bentoml/stable-diffusion-bentoml/blob/main/bentoctl/deployment_config.yaml">deployment_config.yaml</a> 파일에 구성되어 있습니다. 해당 사양들을 자유롭게 업데이트 해주세요. 기본 설정으로 <code class="language-plaintext highlighter-rouge">us-west-1</code> region에 <em>Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) AMI</em>가 있는 <a href="https://aws.amazon.com/ec2/instance-types/g4/">g4dn.xlarge</a> 인스턴스에 Bento가 배포하도록 구성되어 있습니다.</p>

<div class="language-yaml highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="na">api_version</span><span class="pi">:</span> <span class="s">v1</span>
<span class="na">name</span><span class="pi">:</span> <span class="s">stable-diffusion-demo</span>
<span class="na">operator</span><span class="pi">:</span>
  <span class="na">name</span><span class="pi">:</span> <span class="s">aws-ec2</span>
<span class="na">template</span><span class="pi">:</span> <span class="s">terraform</span>
<span class="na">spec</span><span class="pi">:</span>
  <span class="na">region</span><span class="pi">:</span> <span class="s">us-west-1</span>
  <span class="na">instance_type</span><span class="pi">:</span> <span class="s">g4dn.2xlarge</span>
  <span class="c1"># points to Deep Learning AMI GPU PyTorch 1.12.0 (Ubuntu 20.04) 20220913 AMI</span>
  <span class="na">ami_id</span><span class="pi">:</span> <span class="s">ami-0a85a3a3fb34b3c7f</span>
  <span class="na">enable_gpus</span><span class="pi">:</span> <span class="no">true</span>
</code></pre></div></div>

<p>Terraform 파일 생성</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl generate <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<p>Docker 이미지를 만들고 AWS ECR로 push 합니다. 이미지 업로드는 대역폭에 따라 시간이 오래 걸릴 수 있습니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl build <span class="nt">-b</span> stable_diffusion_fp32:latest
</code></pre></div></div>

<p>AWS EC2에 bento를 배포하기 위해 Terraform 파일을 등록합니다. EC2 콘솔에서 브라우저를 퍼블릭 IP 주소로 공개하여 Swagger UI에 접근이 가능합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl apply <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<p>마지막으로 Stable Diffusion BentoML 서비스가 더 이상 필요없다면 배포를 삭제합니다.</p>

<div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>bentoctl destroy <span class="nt">-f</span> deployment_config.yaml
</code></pre></div></div>

<h1 id="결론">결론</h1>

<p>이 글에서 저희는 BentoML을 사용하여 Stable Diffusion을 위한 production-ready 서비스를 구축하고 AWS EC2에 배포했습니다. AWS EC2에 서비스를 배포함으로써 더 강력한 하드웨어에서 Stable Diffusion 모델을 짧은 지연시간으로 이미지를 생성하고 단일 시스템 이상으로 확장할 수 있었습니다. 이 글을 재미있게 읽었다면 <a href="https://github.com/bentoml/BentoML">github의 Bentoml project</a>에 ⭐ 와 <a href="https://l.bentoml.com/join-slack">slack community</a>에서 마음에 맞는 분들을 만나보시길 바랍니다.</p>

<h1 id="reference">Reference</h1>
<p>해당 자료는 BentoML Blog를 번역한 자료 입니다.
<a href="https://modelserving.com/blog/deploying-your-own-stable-diffusion-service-mz9wk">원본 자료</a></p>
:ET